[
  {
    "objectID": "about_en.html",
    "href": "about_en.html",
    "title": "Prof. Pedro Rafael",
    "section": "",
    "text": "Olá, meu nome é Pedro Rafael D. Marinho e sou Professor do Departamento de Estatística da Universidade Federal da Paraíba - UFPB.\n\nSou entusiasta das linguagens de programação R e Julia, porém, também me interesso por outas linguagens.\nAtualmente, tenho interesses na aplicação e no desenvolvimento de bibliotecas computacionais nas áreas de ciência de dados e aprendizagem de máquina. Uma das coisas mais nobres na ciência é desenvolver ferramentas científicas que possam ser amplamente utilizadas na produção e aplicação da ciência.\n\n\n\nBacharel em estatística pela Universidade Federal da Paraíba - UFPB, 2010;\nMestre em estatística pela Universidade Federal de Pernambuco, 2012;\nDoutor em estatística pela Universidade Federal de Pernambuco, 2014.\n\n\n\n\n\n\n\n⬆️ Meu ambiente de trabalho: Departamento de Estatística - UFPB"
  },
  {
    "objectID": "about_en.html#sobre-mim",
    "href": "about_en.html#sobre-mim",
    "title": "Prof. Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOlá, meu nome é Pedro Rafael D. Marinho e sou Professor do Departamento de Estatística da Universidade Federal da Paraíba - UFPB.\n\nSou entusiasta das linguagens de programação R e Julia, porém, também me interesso por outas linguagens.\nAtualmente, tenho interesses na aplicação e no desenvolvimento de bibliotecas computacionais nas áreas de ciência de dados e aprendizagem de máquina. Uma das coisas mais nobres na ciência é desenvolver ferramentas científicas que possam ser amplamente utilizadas na produção e aplicação da ciência.\n\nFormação\n\nBacharel em estatística pela Universidade Federal da Paraíba - UFPB, 2010;\nMestre em estatística pela Universidade Federal de Pernambuco, 2012;\nDoutor em estatística pela Universidade Federal de Pernambuco, 2014."
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#uma-breve-introdução",
    "href": "posts/likelihood_r_julia_python/index.html#uma-breve-introdução",
    "title": "Estimação por máxima verossimilhança em R, Julia e Python",
    "section": "Uma breve introdução",
    "text": "Uma breve introdução\nCiência de dados é, sem dúvidas, uma área de pesquisa que permeia diversas outras ciências, estando mais intimamente relacionada com as áreas de estatística e a computação.\n\n\n\n\n\n\nEu costumo dizer aos meus alunos que um ótimo cientista de dados é o profissional que sabe mais estatística que um cientista da computação mediano e mais computação de que um estatístico mediano.\n\n\n\nTambém é importante dizer, antes de irmos ao tópico desse post, que é possível fazer ciência de dados em qualquer linguagem de programação que você domine! Claro, isso não implica que a produção de ciência de dados é igualmente fácil em qualquer linguagem que você escolha.\n\n\n\n\n\n\nDicas de linguagens\n\n\n\nSe você me permitir te dar uma dica de qual linguagem de programação escolher, sem citar nomes, eu pediria para que você se aproximasse das linguagens que a comunidade que faz ciência de dados estão utilizando! Isso irá facilitar sua vida, pois você terá produtividade, aproveitando da infinidade de bibliotecas e frameworks disponíveis para nossa área.\nPorém, se mesmo assim você quiser insistir na pergunta 😟, me tirando da região de conforto de neutralidade, eu citaria três linguagens para você escolher:\n\nR, veja R Core Team (2022)\nPython, veja Rossum (2010)\nJulia, veja Bezanson et al. (2017)\n\n\n\nSe você está estudando algumas dessas linguagens ou domina ao menos uma delas, certamente você estará traçando um caminho congruente e terá a sua disposição um arsenal de ferramentas prontas para trabalhar com ciência de dados (bibliotecas e frameworks), i.e., você terá vários presentinhos, grátis, para poder utilizar nos seus projetos! 🎁\nCada cientista de dados, por óbvio, tem sua história pessoal, sendo comum trilharem caminhos diferentes na programação. Não se faz ciência de dados sem programação, ok? 👍\nNo meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma década. A linguagem Julia, tive o primeiro contato em 2012, (veja p. 74 de (Bezanson et al. 2017), quando surgiu a linguagem, em 2018 utilizei para construir códigos de um artigo Marinho et al. (2018), mas somente a partir da pandemia de COVID-19 foi que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando mais recentemente, porém, já consigo conversar sobre temas como funções variádicas, estruturas de dados, funções varargs, closures, bibliotecas como NumPy e SciPy, pedras angulares para se fazer ciência de dados em Python, entre outros assuntos. Se você veio de R ou Julia, como foi o meu caso, e quer dominar a linguagem Python, estude a documentação oficial da linguagem que é ótima. Procure conhecer:\n\nAs estruturas de dados de Python: arrays, tuplas, listas, dicionários e conjuntos. Nesse tópico, entenda que tuplas são objetos imutáveis, listas são mutáveis e dicionários são objetos mutáveis, assim como as listas, porém possuem palavras-chave;\nEstude o conceito de funções varargs, técnica muito poderosa, presente em todas às três linguagens, que permite generalizações que conduzem a códigos muito mais úteis;\nEstude o conceito de closures, também presente em todas às três linguagens que estamos conversando, e em diversas outras. Esse conceito permite que suas funções possam construir novas funções;\nSobretudo, experimente a linguagem!\n\n\n\n\n\n\n\nAs linguagens podem conversar entre si 🎉\n\n\n\nVocê não precisará apenas utilizar uma mesma linguagem de programação em um projeto de ciência de dados que esteja trabalhando. Se existe como fazer algo na linguagem que você está utilizando, faça nessa linguagem! Isso irá diminuir o uso de dependências. Porém, caso algo não exista para sua linguagem e você não está afim de implementar algum método do zero, então lembre-se que você poderá importar código de outras linguagens.\nSe você está\n\nem R: você poderá importar, utilizando a biblioteca reticulate, veja Ushey, Allaire, and Tang (2022), métodos e objetos com estruturas de dados em Python onde serão convertidos para estruturas equivalentes em R. Caso queira importar códigos Julia, você poderá utilizar a biblioteca JuliaCall (Li 2019);\nem Julia: você poderá importar códigos R usando a biblioteca RCall. Para importar códigos Python, você deverá utilizar a biblioteca PyCall;\nem Python: você poderá chamar códigos R utilizando a biblioteca rpy2. Já para chamar códigos Julia, veja a biblioteca PyJulia.\n\n\n\nSe você já é fluente em ao menos uma linguagem de programação, nos concentraremos nessas três citadas acima, então perceberá que dominar outra(s) linguagem(ens) será muito mais rápido. São poucas semanas de estudo necessárias para que você já consiga produzir códigos com boas práticas de programação, coisa que foi muito mais árduo no aprendizado da primeira linguagem. Claro, vez ou outra você se pegará olhando com constância os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe é nova para você e eventualmente se misturará 🧠 com a sintaxe das linguagens que você já programa. Normal!\nUm dos problemas corriqueiros para quem trabalha com ciência de dados e, em particular, com inferência estatística é a obtenção dos Estimadores de Máxima Verossimilhança - EMV. Quando estamos utilizando frameworks para ciência de dados, muitas vezes essas estimações ocorrem por baixo do pano. Há diversas situações em que estimadores com boas propriedades estatísticas precisam ser utilizados, e os EMV são, de longe, os mais utilizados.\nNa área de machine learning, por exemplo, se o que está sendo otimizado por “baixo dos panos” não é uma função de log-verossimilhança, existirá alguma função objetivo que precisará ser maximizada ou minimizada, utilizando métodos de otimizações globais; métodos de Newton e quasi-Newton, os mesmos que utilizaremos para obtenção das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na área de redes neurais alimentadas para frente (feedforward), aplicadas a problemas de regressão ou classificação, existe uma função objetivo que considerará os pesos sinápticos da arquitetura da rede com \\(n\\) conexões, isto é, existirá uma função \\(f(y, w_1, w_2, \\cdots)\\), em função dos pesos sinápticos \\(w_i, i = 1, ..., n\\) e da saída esperada. Como \\(y\\) é conhecido (saída esperada da rede), para que a rede esteja bem ajustada, será preciso encontrar um conjunto ótimo de valores \\(w_i\\) que minimize essa função. Isso é feito pelo algoritmo backpropagation utiliza métodos de otimização não-linear para encontrar o mínimo global de uma função objetivo.\nSe você quer conhecer mais profundamente essas metodologias de otimização, escrevi sobre elas no meu material de estatística computacional, na Seção de Otimização Não-linear.\n\n\n\n\n\n\nEm algum momento você tem que saber otimizar funções\n\n\n\nEm fim, você em algum momento, no seu percurso na área de ciência de dados, irá ter que otimizar funções. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma função objetivo. Escolher entre minimizar ⬇️ ou maximizar ⬆️ dependerá da natureza do problema em questão.\nVocê, como um cientista de dados que é, ou que almeja ser, será o responsável em descobrir se o que precisará fazer será maximizar ou minimizar uma função. Sobretudo, você que precisará saber qual função deverá otimizar! Isso vai além da programação. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. 🧠\nFunções objetivos irão sempre aparecer na sua vida! 👍"
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#estimadores-de-máxima-verossimilhança---emv",
    "href": "posts/likelihood_r_julia_python/index.html#estimadores-de-máxima-verossimilhança---emv",
    "title": "Estimação por máxima verossimilhança em R, Julia e Python",
    "section": "Estimadores de máxima verossimilhança - EMV",
    "text": "Estimadores de máxima verossimilhança - EMV\nPara não ficarmos apenas olhando códigos de programação em três linguagens distintas (R, Julia e Python), irei contextualizar um simples problema: o problema de encontrar o máximo da função de verossimilhança. Serei muito breve! 🎉\n\n\n\n\n\n\nA maior barreira 🧱 de uma implementação consistente!\n\n\n\n\n\nA grande barreira que limita a nossa implementação, quando já dominamos ao menos uma linguagem de programação, é não saber ao certo o que desejamos implementar.\nÉ por isso que irei contextualizar, de forma breve, um problema comum na estatística e ciência de dados; o problema de otimizar uma função objetivo. Mais precisamente, desejaremos obter o máximo da função de verossimilhança.\n\n\n\nPara simplificar a teoria, irei considerar algumas premissas:\n\nVocê tem algum conhecimento de probabilidade;\nConsiderarei o caso univariado, em que teremos uma única variável aleatória - v.a. que denotarei por \\(X\\);\nA variável aleatória - v.a. \\(X\\) é contínua, portanto suas observações podem ser modeladas por uma função densidade de probabilidade - fdp \\(f\\), tal que \\(f_X(x) \\geq 0\\) e \\(\\int_{-\\infty}^{+\\infty}f_X(x)\\, \\mathrm{d}x = 1\\).\n\nNa prática, o problema consiste em, por meio de um conjunto de dados, fixarmos uma fdp \\(f_X\\). Daí, desejaremos encontrar os parâmetros de fdp que faz com que \\(f_X\\) melhor se ajuste aos dados. Os parâmetros \\(\\alpha\\) e \\(\\beta\\) que fará com que \\(f_X\\) melhor se ajuste aos dados poderão ser obtidos maximizando a função de verossimilhança \\(\\mathcal{L}\\) de \\(f_X\\), em relação a \\(\\alpha\\) e \\(\\beta\\), definida por:\n\\[\n\\mathcal{L}(x, \\alpha,\\beta) = \\prod_{i = 1}^n f_{X_i}(x, \\alpha, \\beta).\n\\tag{1}\\] Para simplificar as coisas, como \\(\\log(\\cdot)\\) é uma função monótona, então, os valores de \\(\\alpha\\) e \\(\\beta\\) que maximizam \\(\\mathcal{L}\\) serão os mesmos que maximizam \\(\\log(\\mathcal{L})\\), ou seja, poderemos nos concentrar em maximizar:\n\\[\\ell(x, \\alpha, \\beta) = \\sum_{i = i}^n \\log[f_{X_i}(x, \\alpha, \\beta)]. \\tag{2}\\]\nSuponha que \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\), ou seja, que os dados que chegam a sua mesa são provenientes de uma v.a. \\(X\\) que tem observações que seguem a distribuição \\(Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Essa será nossa distribuição verdadeira!\nNa prática, você apenas conhecerá os dados! Será você, como cientista de dados, que irá supor alguma família de distribuições para modelar os dados em questão. 🥴\nVamos supor que você, assertivamente, escolhe a família Weibull de distribuições para modelar o seu conjunto de dados (você fará isso olhando o comportamento dos dados, por exemplo, fazendo um histograma). Existem testes de aderências para checar o quanto uma distribuição se ajusta a um conjunto de dados. Não entraremos nesse assunto aqui!\n\n\nQuer ver o código do gráfico? Clique aqui!\nlibrary(ggplot2)\n\n# Quantidade de elementos\nn <- 550L\n# Parâmetro de forma\nalpha <- 2.5\n# Parâmetro de escala\nbeta <- 1.5\n\n# Fixando uma semente, de forma a sempre obtermos a mesma amostra\nset.seed(0)\ndados <- \n  data.frame(\n    x = seq(0, 2, length.out = n),\n    y_rand = rweibull(n, shape = alpha, scale = beta)\n  )\n\ndados |> \n  ggplot() +\n  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +\n  ggtitle(\n    label = \"Histograma do conjunto de dados\",\n    subtitle = \"Na prática você só tem eles\"\n  ) + \n  labs(\n    y = \"Densidade\",\n    x = \"x\"\n  ) + \n  scale_x_continuous(\n    limits = c(0, 2.7),\n    n.breaks = 15\n  ) + \n  scale_y_continuous(\n    limits = c(0, 1.05),\n    n.breaks = 15\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1),\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.1),\n    color = \"blue\",\n    size = 0.8\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.2),\n    color = \"tomato\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.3),\n    color = \"red\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = beta),\n    color = \"gold\",\n    size = 0.8\n  )\n\n\n\n\n\nNote que todas as funções densidades de probabilidades - fdps plotadas no histograma apresentado no gráfico acima são densidades da família Weibull de distribuições. O que difere uma da outra são os valores de \\(\\alpha\\) e \\(\\beta\\), respectivamente. Não basta escolher uma família de distribuições adequada. Precisamos escolher (estimar) adequadamente os parâmetros que indexam a distribuição, sendo o método de máxima verossimilhança, a metodologia estatística que nos ajudam a fazer uma ótima escolha, conduzindo as estimativas que são provenientes de estimadores com boas propriedades estatísticas.\nLembre-se, para obter essas estimativas, temos que maximizar a Equation 1, ou equivalentemente a Equation 2.\nNo gráfico acima, é possível visualmente perceber que a curva em amarelo é a que melhor aproxima o comportamento dos dados. Não iremos fazer testes de adequação!\n\nDe fato, essa é a curva da distribuição verdadeira, i.e., é a curva da fdp de \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Por sinal, ainda não coloquei a equação da fdp de \\(X\\). Segue logo abaixo:\n\\[f_X(x) = (\\alpha/\\beta)(x/\\beta)^{\\alpha - 1}\\exp[{-(x/\\beta)^\\alpha}],\\] com \\(x\\), \\(\\alpha\\) e \\(\\beta > 0\\).\n\nImplementações\nAgora que já conhecemos \\(f_X\\) e \\(\\ell(\\cdot)\\) (“função objetivo”), poderemos colocar as “mãos na massa” no teclado.\n\n\n\n\n\n\nE os dados?\n\n\n\nOs dados serão gerados aleatoriamente, em cada uma das linguagens (R, Julia e Python). Sendo assim, muito provavelmente não serão os mesmos dados, em cada linguagem, pois a sequência gerada que corresponderá aos nossos dados dependerá das implementações dos geradores de números pseudo-aleatórios de cada linguagem. Porém, os resultados das estimativas devem convergir para valores próximos a \\(\\alpha = 2.5\\) e \\(\\beta = 1.5\\), nas três linguagens. Por isso, não irie comparar, inicialmente, os resultados das estimativas obtidas. Ao final da postagem, farei uma sucinta comparação.\nIrei colocar comentários nos códigos para que você possa estudar cada um deles. Nada em excesso!\n\n\nAntes irei colocar uma observação para a linguagem Python. As pedras angulares para computação científica em Python são as bibliotecas NumPy e Scipy. Por que elas são úteis?\n\nNumpy: é uma biblioteca de código aberto iniciada em 2005 e que possui diversos métodos (funções) numéricas comumente utilizadas na computação científica. Há diversos métodos para operar sobre arrays, vetorização, geração de números pseudo-aleatórios, entre outras coisas. Consulte mais detalhes em https://numpy.org/doc/stable;\nScipy: trata-se de outra biblioteca importante que contém implementações de métodos e algoritmos fundamentais para computação científica, como métodos de integração, interpolação, otimização, entre diversas outras metodologias. Consulte outros detalhes em https://scipy.org.\n\nIremos utilizar ambas as bibliotecas. Basicamente a Numpy será utilizada para vetorização de código, trabalhar com arrays e gerar observações da distribuição Weibull. Nesse último ponto, especificamente, a biblioteca Numpy implementa a função que gera observações de uma distribuição Weibull, onde a distribuição Weibull só tem um parâmetro. Consulte detalhes em https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html.\nNo link você verá que o que é implementado pelo método random.weibull é gerar observações de \\(X = [-\\log(U)]^{1/\\alpha} \\sim Weibull(\\alpha)\\), com \\(U\\) sendo um v.a. uniforme no intervalo (0,1] . Daí, para gerar observações da distribuição \\(Weibull(\\alpha, \\beta)\\), teremos que multiplicar o resultado de random.weibull pelo valor de \\(\\beta\\). Porém, com pouco código, podemos construir uma função para gerar observações da \\(Weibull(\\alpha, \\beta)\\).\n\n\n\nEu olhando para um pouco de malabarismo de código em Python.\n\n\nVeja como seria em R, Julia e Python:\n\nRJuliaPython\n\n\n\nset.seed(0)\nrweibull(n = 10L, shape = 2.5, scale = 1.5)\n\n [1] 0.6181884 1.6792787 1.4930932 1.1870595 0.5881789 1.8107341 0.6138897\n [8] 0.4766274 1.0544363 1.1027820\n\n\n\n\n\nusing Distributions\nusing Random\n\nRandom.seed!(0);\nrand(Weibull(2.5,1.5), 10)\n\n10-element Vector{Float64}:\n 1.3361401397866541\n 1.0507258710483653\n 0.8178793280596004\n 0.6700247875189858\n 0.6429703222489156\n 0.6315072585920245\n 2.313206178975804\n 2.048795844959291\n 1.1773995533033512\n 1.6047832712314394\n\n\n\n\n\nimport numpy as np\n\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha, n)\n\nnp.random.seed(0)\n\nrandom_weibull(n = 10, alpha = 2.5, beta = 1.5)\n\narray([1.36908085, 1.64315124, 1.4528271 , 1.36309319, 1.18186313,\n       1.52263868, 1.20258335, 2.06494277, 2.42259084, 1.12172534])\n\n\n\n\n\n\n\n\n\n\n\nIsso é um pouco estranho, mas tudo bem, sabemos programar!\n\n\n\nTer que multiplicar as observações geradas de uma distribuição que deveria ter, em sua definição, dois parâmetros me parece estranho! Perceba que no código de Python foi preciso fazer definir a função random_weibull, em que foi preciso considerar beta * np.random.weibull(alpha, n) para se ter observações Weibull com valores de \\(\\beta\\) diferente de 1. É fácil adaptar, mais o designer não é legal, na minha opinião.\nAfinal de contas, é muito mais conveniente alterar o comportamento e resultados de uma função passando argumentos para a função, e não fazendo as alterações fora dela. É esse o papel dos argumentos, não? Se o usuário tivesse interesse que \\(\\beta = 1\\), como ocorre em random.weibull ele poderia especificar isso como argumento passados à função, certo?\nComportamentos mais convenientes são observados em R e Julia, afinal de contas, elas surgiram com o foco na computação científica. R é mais voltada para ciência de dados e aprendizagem de máquina. Já Julia, além dos mesmos focos de R, também é uma linguagem de propósito geral, assim como Python é em sua essência.\nNote que essa minha crítica não é a linguagem Python. Refere-se tão somente ao método random.weibull e alguns outros que seguem esse designer de implementação. Python é uma ótima linguagem que vem melhorando o seu desempenho nas novas versões. Veja as novidades de lançamento do Python 3.11, que alcançou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padrão que o comitê de desenvolvimento da linguagem utiliza.\n\n\nAgora, sim, vamos aos três códigos completos para a solução do problema que motiva o título desse post.\n\nNas três linguagens, utilizarei os mesmo conceitos importantes de implementação que conduzem a códigos mais generalizados e a um melhor reaproveitamento de código:\n\nNote que utilizo o conceito de funções com argumentos variáveis, também chamadas de funções varargs. Perceba que a função log_likelihood não precisa ser reimplementada novamente para outras funções densidades. A função densidade de probabilidade é um argumento dessa função que denominei de log_likelihood nos três códigos (R, Python e Julia). Precisamos de funções com operadores varargs, tendo em vista que não conhecemos o número de parâmetros da fdp que o usuário irá passar como argumento. Funções com argumentos varargs é uma técnica muito poderosa. Utilizei esses conceitos em Python e Julia, devido a natureza das funções de otimizações das duas linguagens, em que em seus designers permitem que os argumentos a serem otimizados da função objetivo seja de número variável. As funções varargs de R são definidas pelo operador dot-dot-dot (...). O designer da função optim() de R incluem o parâmetro dot-dot-dot, para argumentos extras que eventualmente possam existir na função objetivo. ⚡\nNote que não é preciso obter analiticamente a expressão da função de log-verossimilhança. Não há sentido nisso, tendo em vista que o nosso objetivo é simplesmente obter as estimativas numéricas para \\(\\alpha\\) e \\(\\beta\\) . É muito mais útil ter uma função genérica que se adéque a diversas outras situações!\nOutro conceito poderoso e que te leva a implementações consistentes é entender o funcionamento das funções anônimas, também conhecidas como funções lambda.\nFoi utilizado como método de otimização (minimização), o método BFGS. Escrevi a respeito dos métodos de quasi-Newton, classe de algoritmos que o método de Broyden–Fletcher–Goldfarb–Shanno - BFGS pertencem, nos materiais que disponibilizo aos meus alunos na disciplina de estatística computacional que leciono no Departamento de Estatística da UFPB. Se quiser um pouco mais de detalhes a respeito dos métodos de Newtone quasi-Newton, clique aqui.\n\n\n\n\n\n\n\nMinimizar ou maximizar?\n\n\n\nAlguns algoritmos de otimização são definidos para minimizar uma função objetivo, como é o caso da maioria das implementações dos métodos de busca global, onde se encaixa o método BFGS. Mas não tem problema, uma vez que minimizar, \\(-f\\) equivale a maximizar \\(f\\), em que \\(f\\) é uma dada função objetivo.\n\n\n\nRJuliaPython\n\n\n\n# Quantidade de observações\nn <- 250L\n\n# Parâmetros que especificam a distribuição verdadeira, i.e., distribuição\n# da variável aleatória cujo os dados são observações.\nalpha <- 2.5\nbeta <- 1.5\n\n# Fixando uma semente para o gerador de números pseudo-aleatórios.\n# Assim, conseguimos, toda vez que rodamos o código, reproduzir \n# os mesmos dados.\nset.seed(0)\n\n# Gerando as observações. Esse será o conjunto de dados que você tera para \n# modelar.\ndados <- rweibull(n = n, shape = alpha, scale = beta)\n\npdf_weibull <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n}\n\n# Checando se a densidade de pdf_weibull integra em 1\narea = integrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))\n\n# Em R, o operador dot-dot-dot (...) é utilizado para definir\n# quantidade variádica de argumentos. Assim, log_likelihood é\n# uma função vararg.\nlog_likelihood <- function(x, pdf, par)\n  -sum(log(pdf(x, par)))\n\nresult <- optim(\n  fn = log_likelihood,\n  par = c(0.5, 0.5),\n  method = \"BFGS\",\n  x = dados,\n  pdf = pdf_weibull\n)\n\n# Imprimindo os valores das estimativas de máxima verossimilhança\ncat(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\ncat(\"--> alpha: \", result$par[1], \"\\n\")\n\n--> alpha:  2.834629 \n\ncat(\"--> beta: \", result$par[2], \"\\n\")\n\n--> beta:  1.456748 \n\n\n\n\n\nusing Distributions\nusing Random\nusing Optim\nusing QuadGK\n\n# Quantidade de observações\nn = 250;\n\n# Parâmetros que especificam a distribuição verdadeira, i.e., \n# da fdp da v.a. cujos os dados são observações.\nα = 2.5;\nβ = 1.5;\n\n# Fixando um valor de semente\nRandom.seed!(0);\n\n# Gerando um array de dados com distribuição Weibull(α, β)\ndados = rand(Weibull(α,β), n);\n\n# Função densidade de probaiblidade de uma v.a. \n# X ∼ Weibull(α, β)\nfunction pdf_weibull(x, par = (α, β))\n    α, β = par.α, par.β   \n    @. α/β * (x/β)^(α-1) * exp(-(x/β)^α)\nend;\n\n# Checando se a integral no suporte de pdf_weibull integra em 1\narea, error = quadgk(x -> pdf_weibull(x, (α = α, β = β)), 0, Inf);\n\n# Escrevendo a função log_likelihood que em julia denotarei por\n# ℓ, tendo em vista que podemos fazer uso de caracteres UTF-8 \n# nessa linguagem. \nfunction ℓ(x, pdf, par...)\n    -sum(log.(pdf(x, par...)))\nend;\n\n# Encontrando as estimativas de máxima verossimilhança usando a\n# biblioteca Optim\nemv = optimize(\n        x -> ℓ(dados, pdf_weibull, (α = x[1], β = x[2])),\n        [0.5, 0.5],\n        LBFGS()\n); \n\nemv_α, emv_β  = emv.minimizer;\n\n# Imprimindo o resultado\n\nprint(\"Valores estimados para α e β\\n\")\n\nValores estimados para α e β\n\nprint(\"--> α: \", emv_α, \"\\n\")\n\n--> α: 2.4472234393032837\n\nprint(\"--> β: \", emv_β, \"\\n\")\n\n--> β: 1.4480344422787308\n\n\n\n\n\nimport numpy as np\nimport scipy.stats as stat\nimport scipy.integrate as inte\nimport scipy.optimize as opt\n\n# Valores da distribuição verdadeira\nalpha = 2.5\nbeta = 1.5\n\n# Número de observações que irão compor nossos dados\nn = 250 \n\n# Implementando a função random_weibull, em que os parâmetros\n# que indexam a distribuição são argumentos da função. Tem mais\n# sentido ser assim, não?\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha,n)\n\n# Escrevendo a funçãp densidade de probabilidade da Weibull\n# na reparametrização correta.\ndef pdf_weibull(x, param):\n  alpha = param[0]\n  beta = param[1]\n  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)\n\n# Testando se a densidade integra em 1\nárea = round(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),\n      0, np.inf, args = (1,1))[0],2)\n\n# Implementando uma função genérica que implementa a função objetivo\n# (função de log-verossimilhança) que iremos maximizar. Essa função \n# irá receber como argumento uma função densidade de probabilidade.\n# Não é preciso destrinchar (obter de forma exata) a função de \n# log-verossimilhança!\n# A função de log-verossimilhança encontra-se multiplicada por -1\n# devido ao fato da função que iremos fazer otimização minimizar \n# uma função função objetivo. Minimizar -f equivale a maximizar f.\n# Lembre-se disso!\ndef log_likelihood(x, pdf, *args):\n  return -np.sum(np.log(pdf(np.array(x), *args)))\n\n# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa \n# é nossa distribuição verdadeira, i.e., é a distribuição que gera\n# que gerou os dados que desejamos ajustar.\n# Precisamos fixar uma semente, uma vez que queremos os mesmos dados\n# toda vez que rodamos esse código. \nnp.random.seed(0)\ndados = random_weibull(n = n, alpha = alpha, beta = beta)\n\n# Miminimizando a função -1 * log_likelihood, i.e., maximizando\n# a função log_likelihood.\nalpha, beta = opt.minimize(\n  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),\n  x0=[0.5, 0.5]\n).x\n\n# Imprimindo os valores das estimativas de máxima verossimilhança\nprint(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\nprint(\"--> alpha: \", alpha, \"\\n\")\n\n--> alpha:  2.541634329050904 \n\nprint(\"--> beta: \", beta, \"\\n\")\n\n--> beta:  1.480430581806816 \n\n\n\n\n\n\n\nVisualização gráfica\nVamos agorar comparar graficamente as estimativas obtidas pelos métodos de otimização (BFGS) considerando as bibliotecas fornecidas nas três linguages. Para uma melhor visualização, utilizaremos o gráfico de curvas de níveis. Perceba que \\(\\mathcal{L}\\) possui como variáveis os parâmetros \\(\\alpha\\) e \\(\\beta\\), sendo \\(x\\) o conjunto de dados que é fixo. Assim, consiguiremos visualizar em 2D o comportamento de \\(\\mathcal{L}\\).\nPara que possamos visualizar graficamente e comparar as estimativas obtidas nas três linguagens, será preciso que o conjunto de dados seja idêntico nos processos de otimização realizados em R, Julia e Python.\nPara acessar o conjunto de dados, clique aqui. O conjunto de dados possui \\(n = 500\\) observações. Ainda continuarei considerando \\(\\alpha = 2.5\\) e \\(\\beta = 1.5\\).\n\n\nQuer ver o código do gráfico? Clique aqui!\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(latex2exp)\n\nset.seed(0)\n\ndados <- rweibull(n = 500L, shape = 2.5, scale = 1.5)\n\npdf_weibull <- function(x, alpha, beta)\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n\n# log-verossimilhança\nlog_likelihood <- function(x, alpha, beta)\n  prod(pdf_weibull(x, alpha, beta))\n\nvec_log_likelihood <- \n  Vectorize(\n    FUN = log_likelihood,\n    vectorize.args = c(\"alpha\", \"beta\")\n  )\n\nalpha <- seq(2.4, 2.87, length.out = length(dados))\nbeta <- seq(1.42, 1.56, length.out = length(dados))\ndf_contour <- expand_grid(alpha, beta)\n\ndf_contour <- \n  df_contour |> \n  mutate(z = vec_log_likelihood(x = dados, alpha, beta))\n  \ndf_contour |> \n  ggplot() + \n  geom_contour_filled(aes(x = alpha, y = beta, z = z), show.legend = FALSE) +\n  ggtitle(\n    label = \"Curvas de níveis da fução de Verossimilhança\", \n    subtitle = \"Comparativo R, Python e Julia\" \n  ) +\n  xlab(TeX(r'(\\alpha)')) +\n  ylab(TeX(r'(\\beta)')) +\n  labs(fill = \"Níveis\") +\n  theme(\n    plot.title  = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ) +\n  # R\n  geom_point(\n    x = 2.64276,\n    y = 1.488277 ,\n    color = \"blue\",\n    size = 3\n  ) +\n  # Python\n  geom_point(\n    x = 2.642850353244295,\n    y = 1.488271661049081,\n    color = \"green\",\n    size = 2.5\n  ) + \n  # Julia\n  geom_point(\n    x = 2.642850346755736,\n    y = 1.4882716649411558 ,\n    color = \"red\",\n    size = 2\n  )\n\n\n\n\n\nPerceba que foi obtido ótimas estimativas pelo método BFGS, nas três linguagens. Note que tive que colocar um ponto maior que o outro, para que eles não ficassem totalmente sobrepostos, e assim pudéssemos visualizar."
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#conclusões",
    "href": "posts/likelihood_r_julia_python/index.html#conclusões",
    "title": "Estimação por máxima verossimilhança em R, Julia e Python",
    "section": "Conclusões",
    "text": "Conclusões\nEspero que esse post tenha conseguido exemplificar como poderemos implementar a solução de um problema de estimação por máxima verossimilhança, utilizando as linguagens de programação R, Python e Julia. Abordamos conceitos interessantes como funções varargs e funções anônimas, além de algumas estruturas de dados e bibliotecas.\nNos primeiros códigos, foram observados ótimas estimativas, muito embora elas não são comparáveis, tendo em vista que os conjuntos de dados gerados aleatoriamente foram distintos, por conta da natureza de implementação das funções para geração de números pseudo-aleatórios, disponíveis em cada uma das linguagens comparadas. Foi escolhido fazer dessa forma, para que fosse possível abordar o problema de geração de números pseudo-aleatórios em cada linguagem.\nNo fim, foi realizado uma comparação mais justa, onde o mesmo conjunto de dados foi considerado para a produção das curvas de níveis, úteis para visualizar em uma imagem 2D a qualidade das estimativas. No gráfico de curvas de níveis, é possível observar a sobreposição das estimativas obtidas utilizando R, Python e Julia.\nTambém é possível perceber que otimizar uma função objetivo é fácil, independentemente da linguagem, sendo possível conseguir códigos concisos e eficientes.\nTodos os gráficos dessa postagem foram construídos em R, através da biblioteca Wickham (2016). Caso queira construir gráficos em Python ou em Julia, considere as seguintes cito a biblioteca seaborn de Python e a biblioteca Makie de Julia. São apenas sugestões, tendo em vista que há diversas outras alternativas interessantes."
  },
  {
    "objectID": "about_pt_br.html",
    "href": "about_pt_br.html",
    "title": "Prof. Pedro Rafael",
    "section": "",
    "text": "⬆️ Meu ambiente de trabalho: Departamento de Estatística - UFPB"
  },
  {
    "objectID": "about_pt_br.html#sobre-mim",
    "href": "about_pt_br.html#sobre-mim",
    "title": "Prof. Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOlá, meu nome é Pedro Rafael D. Marinho e sou Professor do Departamento de Estatística da Universidade Federal da Paraíba - UFPB.\n\nSou entusiasta das linguagens de programação R e Julia, porém, venho me interessando por outras linguagens, como é o caso de Python.\nAtualmente, tenho interesses na aplicação e no desenvolvimento de bibliotecas computacionais nas áreas de ciência de dados e aprendizagem de máquina. Uma das coisas mais nobres na ciência é desenvolver ferramentas científicas que possam ser amplamente utilizadas na produção e aplicação da ciência.\n\nFormação\n\nBacharel em estatística pela Universidade Federal da Paraíba - UFPB, 2010;\nMestre em estatística pela Universidade Federal de Pernambuco, 2012;\nDoutor em estatística pela Universidade Federal de Pernambuco, 2014."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prof. Pedro Rafael",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\n \n\n\n\n\nOct 6, 2022\n\n\nProf. Pedro Rafael D. Marinho\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\nPython\n\n\nJulia\n\n\n \n\n\n\n\nSep 9, 2022\n\n\nProf. Pedro Rafael D. Marinho\n\n\n22 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/serie_de_taylor/index.html",
    "href": "posts/serie_de_taylor/index.html",
    "title": "Série de Taylor, código espaguete e outras coisas",
    "section": "",
    "text": "Uma boa forma de desenvolver fluência em uma linguagem de programação é se submeter a resolver problemas utilizando a linguagem que você deseja aprender. Para quem tem interesse em trabalhar com computação científica, um exercício muito interessante é selecionar alguns problemas básicos na sua área de atuação e tentar resolver! Tente entender bem o problema a ser resolvido e se submeta ao exercício de implementar uma função da forma mais genérica possível.\nEsse tópico sugiro meio que por acaso. Na verdade, eu estava na sala de aula, explicando aos meus alunos sobre funcionais em R e dizia a eles que nem sempre precisamos utilizar instruções de repetição como for, while e repeat para repetir trechos de códigos, e na aula usei o exemplo das séries de Taylor. Tanto na aula, quanto aqui, deixo claro que séries de Taylor não é o tema principal, muito embora o título da postagem possa talvez te confundir a respeito do propósito dessa postagem.\nO maior objetivo é fazer com que você possa estudar o código e entender os recursos utilizados para se chegar a solução. Procure entender o papel do funcional sapply(), da função do.call(), do operador dot-dot-dot ... e do uso de função anônima no funcional sapply(). Esses são basicamente os principais pontos do código.\nTodavia, é claro que eu terei que definir brevemente o que é uma série de Taylor. Afinal, precisamos compreender, de forma clara, o que o código tenta implementar. Tudo bem?! 🤝\nCompreender de forma cristalina o problema é a chave 🔐 para desenvolver, de forma consistente, um código coerente e reaproveitável. Tente evitar o código espaguete, em que você até resolve o problema, porém, o reaproveitamento de código e a manutenção do mesmo se torna muito desafiador! Fuja desses tipos de implementações e projetos, pois estes não acrescentam muito no seu aprendizado de programação.\nUma ótima postagem que encontrei na internet e que poderá ser útil para você entender sobre a importância de se afastar de código espaguete, em R, poderá ser acessada clicando AQUI. E não se engane, apesar do termo engraçado (código espaguete), há diversos livros na computação sobre o tema."
  },
  {
    "objectID": "posts/serie_de_taylor/index.html#série-de-taylor-em-r",
    "href": "posts/serie_de_taylor/index.html#série-de-taylor-em-r",
    "title": "Implementando uma série de Taylor no R",
    "section": "Série de Taylor em R",
    "text": "Série de Taylor em R\nUma boa forma de desenvolver fluência em uma linguagem de programação é programando."
  },
  {
    "objectID": "posts/serie_de_taylor/index.html#série-de-taylor",
    "href": "posts/serie_de_taylor/index.html#série-de-taylor",
    "title": "Série de Taylor, código espaguete e outras coisas",
    "section": "Série de Taylor",
    "text": "Série de Taylor\nEm matemática, uma série de Taylor é uma soma de funções, de tal forma que\n\\[f(x) = \\sum_{n = 0}^{\\infty} a_n(x-a)^n,\\] com\n\\[a_n = \\frac{f^{(n)}(a)}{n!},\\] em que \\(f^{(n)}(a)\\) é a \\(n\\)-ésima derivada da função \\(f\\) (supostamente sendo infinitamente diferenciável) avaliada no ponto \\(a\\). Para o caso de \\(a = 0\\), essa aproximação é denominada de série de Maclaurin.\nPor exemplo, considerado \\(a = 0\\), poderemos aproximar \\(f(x) = e^x\\), em torno de \\(a = 0\\) por meio da expressão abaixo:\n\\[e^x \\approx \\sum_{n = 0}^N \\frac{x^n}{n!},\\] com \\(N < \\infty\\).\n\n\n\n\n\nNote que temos uma ótima aproximação de \\(f(x)\\), em torno de \\(a = 0\\), por meio do polinômio de Taylor, considerando \\(N = 2\\)."
  },
  {
    "objectID": "posts/serie_de_taylor/index.html#fuja-de-códigos-espaguetes",
    "href": "posts/serie_de_taylor/index.html#fuja-de-códigos-espaguetes",
    "title": "Implementando uma série de Taylor no R",
    "section": "Fuja de códigos espaguetes 🍝",
    "text": "Fuja de códigos espaguetes 🍝\nAtualmente, o meu maior pavor em ajudar alguém em programação é olhar para o código de uma pessoa e ver nele um espaguete com códigos confusos e escrito de forma “enlinhada” (como um espaguete 🍝) e cheios de gambiarras."
  },
  {
    "objectID": "posts/serie_de_taylor/index.html#implementação",
    "href": "posts/serie_de_taylor/index.html#implementação",
    "title": "Série de Taylor, código espaguete e outras coisas",
    "section": "Implementação",
    "text": "Implementação\nNão irei discutir, nos por menores, a implementação do código, pois certamente seria enfadonho. Estudar o código “não espaguete” 🍝 de outra pessoa é uma boa forma de adquirir conhecimento. O GitHub está aí para você estudar um universo de códigos e melhorar suas práticas de programação, seja em R ou qualquer outra linguagem de programação.\nO que eu basicamente fiz foi implementar duas funções:\n\nA função taylor(n = 1L, func, x, a = 0,...) que recebe o número de somas n, a função func a ser aproximada, um valor x e o valor de a. Também é possível passar argumentos adicionais usando o operador varargs dot-dot-dot (...), em que você poderá controlar argumentos da função Deriv() do pacote Deriv utilizado para o cálculo das derivadas simbólicas de \\(f(x)\\). O pacote encontra-se no Comprehensive R Archive Network - CRAN e poderá ser instalado fazendo install.packages(\"Deriv\")(Clausen and Sokol 2020);\nA função plot_taylor(n, func, lower, upper, a) que permitirá que possamos visualizar graficamente a aproximação. Temos que n é a quantidade de somas utilizadas na aproximação, func é a função que desejamos aproximar, lower é o limite inferior do eixo x, upper é o limite superior do eixo \\(x\\) e a é o ponto onde tentaremos uma boa aproximação em seu entorno.\n\nNote que, em R, funções são objetos de primeira classe. Isso quer dizer que, podemos criar uma função dentro de outra função e, sobretudo, poderemos ter uma função retornando uma função, como é o caso da função an() definida em taylor(). Note que a função an() retorna a função que implementa a derivada de func.\n\n# Série de Taylor ---------------------------------------------------------\ntaylor <- function(n = 1L, func, x, a = 0,...){\n  an <- function(n, ...){\n    Deriv::Deriv(f = func, nderiv = n, ...)\n  }\n  \n  if(n == 1L)\n    return(func(a))\n  \n  sapply(\n    X = 1L:n,\n    FUN = \\(n, x) do.call(an(n, ...), list(x = x))/factorial(n) * (x - a)^n,\n    x = x\n  ) |> sum() + func(a) \n}\n\n# Testando a função taylor(). \ntaylor(n = 2L, func = \\(x) x^2, x = 0.5, a = 0)\n\n[1] 0.75\n\n\n\n\n\n\n\n\nDerivadas simbólicas\n\n\n\nConhecer derivadas de funções é algo muito corriqueiro na computação científica. Com o pacote Deriv você não precisa se preocupar em derivar simbolicamente (analiticamente) as \\(n\\)-ésimas derivadas de uma função.\n\nlibrary(Deriv)\n\n# Derivando simbolicamente log(1-sin(x)). Perceba que\n#  o retorno é uma função R em termos de x.\nDeriv(f = \\(x) log(1-sin(x)))\n\nfunction (x) \n-(cos(x)/(1 - sin(x)))\n\n\nÉ importante destacar que a obtenção de derivadas simbólicas são mais custosas que os métodos numéricos de derivação, sobretudo quando temos interesse de obter ordens elevadas de derivação. Mesmo assim, irei utilizar aqui a biblioteca Deriv, que poderá ser bastante útil em diversas situações. Para a obtenção de derivadas numéricas, estude a biblioteca numDeriv (Gilbert and Varadhan 2019).\n\n\nVisualizar graficamente os resultados de uma função poderá nos ajudar na validação da implementação. Comportamentos inadequados na função taylor() seriam facilmente percebidos ao utilizar a função plot_taylor().\n\nlibrary(ggplot2)\nlibrary(glue)\nlibrary(patchwork)\n\nplot_taylor <- function(n = 1L, func, a, lower = -1, upper = 1, ...){\n  x <- seq(lower, upper, length.out = 100L)\n  y <- func(x)\n  \n  y_taylor <- \n    sapply(\n      X = x,\n      FUN = \\(x) taylor(n = n, func = func, x = x, a = a, ...)\n    )\n  \n  data.frame(\n    x = c(x, x), \n    y = c(y, y_taylor),\n    classe = c(rep(\"f(x)\", 100L), rep(\"Taylor\", 100L))\n  ) |> \n    ggplot() +\n    geom_line(aes(x = x, y = y, color = classe), size = 0.9) +\n    geom_point(x = a, y = func(a), color = \"blue\", size = 2) +\n    ggtitle(\n      label = \"Série de Taylor\", \n      subtitle = glue(\"Aproximação de uma função f(x)\\n n = {n}\") \n    ) +\n    ylab(\"f(x)\") +\n    scale_color_manual(values = c(\"black\", \"tomato\")) + \n    theme(\n      text = element_text(face = \"bold\")\n    ) + \n    labs(color = \"\") \n}\n\n# Aproximando func em torno de \"a\" ----------------------------------------\np1 <- plot_taylor(\n  n = 3,\n  func = \\(x) exp(x),\n  lower = -1,\n  upper = 1,\n  a = 0.5\n)\n\np2 <- plot_taylor(\n  n = 3,\n  func = \\(x) x^3,\n  lower = -1,\n  upper = 1,\n  a = 0.5\n)\n\np3 <- plot_taylor(\n  n = 3,\n  func = \\(x) 3*x^3 + 4*x^2,\n  lower = -1,\n  upper = 1,\n  a = 0\n)\n\np4 <- plot_taylor(\n  n = 3,\n  func = \\(x) 15*x^2 - 14*x + 7,\n  lower = -0.8,\n  upper = 2,\n  a = 1.5\n)\n\n(p1 + p2)/(p3 + p4) + plot_annotation(tag_levels = \"I\")"
  }
]