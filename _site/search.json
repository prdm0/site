[
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#uma-breve-introdu√ß√£o",
    "href": "posts/likelihood_r_julia_python/index.html#uma-breve-introdu√ß√£o",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Uma breve introdu√ß√£o",
    "text": "Uma breve introdu√ß√£o\nCi√™ncia de dados √©, sem d√∫vidas, um √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.\n\n\n\n\n\n\nEu coscumo dizer aos meus alunos que um √≥timo cientista de dados √© o profissional que sabe mais estat√≠stica que um bom cientista da computa√ß√£o e mais computa√ß√£o de que um bom estat√≠stico.\n\n\n\nTamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine. Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.\n\n\n\n\n\n\nDicas de linguagens\n\n\n\nSe voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando.\nPor√©m, se mesmo assim voc√™ quiser insistir na pergunta, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:\n\nR\nPython\nJulia\n\n\n\nSe voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados.\nCada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal e √© comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o!\nNo meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, quando surgiu a linguagem, mas somente a partir da pandemia de COVID-19 que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando muito recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas (fun√ß√µes varargs), closures, bibliotecas como NumPy e SciPy, que s√£o pedras angulares para se fazer ci√™ncia de dados em Python.\nSe voc√™ j√° √© fluente em ao menos um linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar uma outra linguagem √© muito mais r√°pido. S√£o poucas semanas para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° com a sintaxe das linguagens que voc√™ j√° programa.\nUm dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos Estimadores de M√°xima Verossimilhan√ßa - EMV. Quando estamos utilizando frameworks de machine learning, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.\nNa √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por ‚Äúbaixo dos panos‚Äù n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que percisa ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para oben√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward) aplicadas √† problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo que levar√° em considera√ß√£o os pesos sin√°pticos da arquitetura da rede com \\(n\\) conex√µes, isto √©, existir√° uma fun√ß√£o \\(f(y, w_1, w_2, \\cdots)\\), em fun√ß√£o dos pesos sin√°piticos \\(w_i, i = 1, ..., n\\) e da saida esperada. Como \\(y\\) √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontar um conjunto √≥timo de valores \\(w_i\\) que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo backpropagation que tamb√©m faz uso de m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global da fun√ß√£o objetivo.\nSe voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de estat√≠stica computacional, no Se√ß√£o de Otimiza√ß√£o N√£o-linear.\n\n\n\n\n\n\nEm algum momento voc√™ tem que saber otimizar fun√ß√µes\n\n\n\nEm fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema.\nVoc√™, como um cientista de dados que √© ou que almeja ser √© que dever√° saber se o que precisa √© maximizar ou minimizar. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. ‚úåÔ∏è\nFun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üòÖ"
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#estimadores-de-m√°xima-verossimilhan√ßa---emv",
    "href": "posts/likelihood_r_julia_python/index.html#estimadores-de-m√°xima-verossimilhan√ßa---emv",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Estimadores de m√°xima verossimilhan√ßa - EMV",
    "text": "Estimadores de m√°xima verossimilhan√ßa - EMV\nPara que n√£o fiquemos apenas olhado c√≥digos de programa√ß√£o em tr√™s linguagens distintas (R, Julia e Python), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da fun√ß√£o de verossimilhan√ßa. Serei muito breve. üéâ\n\n\n\n\n\n\nA maior barreira üß± de uma implementa√ß√£o consistente!\n\n\n\n\n\nA grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o √© n√£o saber ao certo o que desejamos implementar.\n√â por isso que irei contextualizar, de forma breve, um comum na estat√≠stica e ci√™ncia de dados que √© o problema de otimizar uma fun√ß√£o objetivo, mais precisamente, obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.\n\n\n\nPara simplificar a teoria, irei considerar algumas premissas:\n\nVoc√™ tem algum conhecimento de probabilidade;\nConsiderarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por \\(X\\);\nA vari√°vel aleat√≥ria - v.a. \\(X\\) √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma fun√ß√£o densidade de probabilidade - fdp \\(f\\), tal que \\(f_X(x) \\geq 0\\) e \\(\\int_{-\\infty}^{+\\infty}f_X(x)\\, \\mathrm{d}x = 1\\).\n\nNa pr√°tica, o problema consiste em, atrav√©s de um conjunto de dados, fixarmos uma fdp \\(f_X\\). Da√≠, desejamos encontrar os par√¢metros de fdp que faz com que \\(f_X\\) melhor se ajuste aos dados. Os par√¢metros \\(\\alpha\\) e \\(\\beta\\) que far√° com que \\(f_X\\) melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa \\(\\mathcal{L}\\) de \\(f_X\\), em rela√ß√£o a \\(\\alpha\\) e \\(\\beta\\), definida por:\n\\[\n\\mathcal{L}(x, \\alpha,\\beta) = \\prod_{i = 1}^n f_{X_i}(x, \\alpha, \\beta).\n\\tag{1}\\] Para simplificar as coisas, como \\(\\log(\\cdot)\\) √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de \\(\\alpha\\) e \\(\\beta\\) que maximizam \\(\\mathcal{L}\\) ser√£o os mesmos que maximizam \\(\\log(\\mathcal{L})\\), ou seja, poderemos nos concentrar em maximizar:\n\\[\\ell(x, \\alpha, \\beta) = \\sum_{i = i}^n \\log[f_{X_i}(x, \\alpha, \\beta)]. \\tag{2}\\]\nSuponha que \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\), ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. \\(X\\) que tem observa√ß√µes que segue a distribui√ß√£o \\(Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Essa ser√° nossa distribui√ß√£o verdadeira!\nNa pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥\nVamos supor que voc√™, acertivamente, escolhe a fam√≠la Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma).\n\n\nQuer ver o c√≥digo do gr√°fico? Clique aqui!\nlibrary(ggplot2)\n\n# Quantidade de elementos\nn <- 550L\n# Par√¢metro de forma\nalpha <- 2.5\n# Par√¢metro de escala\nbeta <- 1.5\n\n# Fixando uma semente, de forma a sempre obtermos a mesma amostra\nset.seed(0)\ndados <- \n  data.frame(\n    x = seq(0, 2, length.out = n),\n    y_rand = rweibull(n, shape = alpha, scale = beta)\n  )\n\ndados |> \n  ggplot() +\n  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +\n  ggtitle(\n    label = \"Histograma do conjunto de dados\",\n    subtitle = \"Na pr√°tica voc√™ s√≥ tem eles\"\n  ) + \n  labs(\n    y = \"Densidade\",\n    x = \"x\"\n  ) + \n  scale_x_continuous(\n    limits = c(0, 2.7),\n    n.breaks = 15\n  ) + \n  scale_y_continuous(\n    limits = c(0, 1.05),\n    n.breaks = 15\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1),\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.1),\n    color = \"blue\",\n    size = 0.8\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.2),\n    color = \"tomato\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.3),\n    color = \"red\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = beta),\n    color = \"gold\",\n    size = 0.8\n  )\n\n\n\n\n\nNote que todas as fun√ß√µes densidades de probabilidas - fdps plotadas no histograma aprensentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de \\(\\alpha\\) e \\(\\beta\\), respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequadade. Precisamos escolher (estimar) adequadade os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.\nLembre-se, para obter essas estimativas, temos que maximizar a Equation¬†1, ou equivalentemente a Equation¬†2.\nNo gr√°fico acima, √© possivel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. De fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de \\(X\\). Segue logo abaixo:\n\\[f_X(x) = (\\alpha/\\beta)(x/\\beta)^{\\alpha - 1}\\exp[{-(x/\\beta)^\\alpha}],\\] com \\(x\\), \\(\\alpha\\) e \\(\\beta > 0\\).\n\nImplementa√ß√µes\nAgora que j√° conhecemos \\(f_X\\) e \\(\\ell(\\cdot)\\) (‚Äúfun√ß√£o objetivo‚Äù), poderemos colocar as ‚Äúm√£os na massa‚Äù no teclado.\n\n\n\n\n\n\nE os dados?\n\n\n\nOs dados ser√£o gerados aleatoriamente, em cada uma das linguagem (R, Julia e Python). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° da implementa√ß√£o dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a \\(\\alpha = 2.5\\) e \\(\\beta = 1.5\\), nas tr√™s linguagens.\nIrei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles.\n\n\nAntes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?\n\nNumpy: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£om de n√∫meros pseudo atelat√≥rios, entre outras coisas. Consulte mais detalhes em https://numpy.org/doc/stable;\nScipy: tarta-se de uma outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos de algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integrama√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em https://scipy.org.\n\nIremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificiamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html.\nNo link voc√™ ver√° que o que √© implementado pelo m√©todo random.weibull √© gerar observa√ß√µes de \\(X = [-\\log(U)]^{1/\\alpha} \\sim Weibull(\\alpha)\\), com \\(U\\) sendo um v.a. uniforme no intervalo (0,1] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o \\(Weibull(\\alpha, \\beta)\\), teremos que multiplicar o resultado de random.weibull pelo valor de \\(\\beta\\). Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da \\(Weibull(\\alpha, \\beta)\\).\nVeja como seria em R, Julia e Python:\n\nRJuliaPython\n\n\n\nset.seed(0)\nrweibull(n = 10L, shape = 2.5, scale = 1.5)\n\n [1] 0.6181884 1.6792787 1.4930932 1.1870595 0.5881789 1.8107341 0.6138897\n [8] 0.4766274 1.0544363 1.1027820\n\n\n\n\n\nusing Distributions\nusing Random\n\nRandom.seed!(0);\nrand(Weibull(2.5,1.5), 10)\n\n10-element Vector{Float64}:\n 1.3361401397866541\n 1.0507258710483653\n 0.8178793280596004\n 0.6700247875189858\n 0.6429703222489156\n 0.6315072585920245\n 2.313206178975804\n 2.048795844959291\n 1.1773995533033512\n 1.6047832712314394\n\n\n\n\n\nimport numpy as np\n\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha, n)\n\nnp.random.seed(0)\n\nrandom_weibull(n = 10, alpha = 2.5, beta = 1.5)\n\narray([1.36908085, 1.64315124, 1.4528271 , 1.36309319, 1.18186313,\n       1.52263868, 1.20258335, 2.06494277, 2.42259084, 1.12172534])\n\n\n\n\n\n\n\n\nEu olhando param um pouco de malabarismo de c√≥digo (desnecess√°rio) em Python.\n\n\n\n\n\n\n\n\nIsso √© um pouco estranho, mas tudo bem, sabemos programar!\n\n\n\nTer que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o random_weibull, em que foi preciso considerar beta * np.random.weibull(alpha, n) para se ter observa√ß√µes Weibull com valores de \\(\\beta\\) diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.\nAfinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que \\(\\beta = 1\\), como ocorre em random.weibull ele poderia especificar isso como argumento da fun√ß√£o, certo?\nComportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de propr√≥sito geral, assim como Python √© em sua ess√™ncia.\nNote que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo random.weibull e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as novidades de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.\n\n\nAgora sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.\n\nNas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamente de c√≥digo:\n\nNote que utilizo o conceito de fun√ß√µes com argumentos vari√°dicos, tamb√©m chamadas de fun√ß√µes varargs. Perceba que a fun√ß√£o log_likelihood n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de log_likelihood nos tr√™s c√≥digos (R, Python e Julia). Precisamos de operadores varargs, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos varargs √© uma t√©cnica muito poderosa. ‚ö°\nNote que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para \\(\\alpha\\) e \\(\\beta\\) . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se adeque a diversas outras situa√ß√µes!\nOutro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecida como **fun√ß√µes lambda**.\nFoi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo BFGS. Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno - BFGS pertencem, nos materiais que diponibilizo aos meus alunos na discipina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes, clique aqui.\n\n\n\n\n\n\n\nMinimizar ou maximizar?\n\n\n\nAlguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, \\(-f\\) equivale a maximizar \\(f\\), em que \\(f\\) √© uma dada fun√ß√£o objetivo.\n\n\n\nRPython\n\n\n\n# Quantidade de observa√ß√µes\nn <- 250L\n\n# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o\n# da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes.\nalpha <- 2.5\nbeta <- 1.5\n\n# Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios.\n# Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir \n# os mesmos dados.\nset.seed(3)\n\n# Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para \n# modelar.\ndados <- rweibull(n = n, shape = alpha, scale = beta)\n\npdf_weibull <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n}\n\n# Checando se a densidade de pdf_weibull integra em 1\nintegrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))\n\n1 with absolute error < 6e-06\n\n# Em R, o operador dot-dot-dot (...) √© utilizado para definir\n# quantidade vari√°dica de argumentos. Assim, log_likelihood √©\n# uma fun√ß√£o vararg.\nlog_likelihood <- function(x, pdf, par)\n  -sum(log(pdf(x, par)))\n\nresult <- optim(\n  fn = log_likelihood,\n  par = c(0.5, 0.5),\n  method = \"BFGS\",\n  x = dados,\n  pdf = pdf_weibull\n)\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\ncat(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\ncat(\"--> alpha: \", result$par[1], \"\\n\")\n\n--> alpha:  2.86476 \n\ncat(\"--> beta: \", result$par[2], \"\\n\")\n\n--> beta:  1.503992 \n\n\n\n\n\nimport numpy as np\nimport scipy.stats as stat\nimport scipy.integrate as inte\nimport scipy.optimize as opt\n\n# Valores da distribui√ß√£o verdadeira\nalpha = 2.5\nbeta = 1.5\n\n# N√∫mero de observa√ß√µes que ir√£o compor nossos dados\nn = 250 \n\n# Implementando a fun√ß√£o random_weibull, em que os par√¢metros\n# que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais\n# sentido ser assim, n√£o?\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha,n)\n\n# Escrevendo a fun√ß√£p densidade de probabilidade da Weibull\n# na reparametriza√ß√£o correta.\ndef pdf_weibull(x, param):\n  alpha = param[0]\n  beta = param[1]\n  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)\n\n# Testando se a densidade integra em 1\nround(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),\n      0, np.inf, args = (1,1))[0],2)\n\n# Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo\n# (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o \n# ir√° receber como argumento uma fun√ß√£o densidade de probabilidade.\n# N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de \n# log-verossimilhan√ßa!\n# A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1\n# devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar \n# uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f.\n# Lembre-se disso!\n\n1.0\n\ndef log_likelihood(x, pdf, *args):\n  return -np.sum(np.log(pdf(np.array(x), *args)))\n\n# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa \n# √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera\n# que gerou os dados que desejamos ajustar.\n# Precisamos fixar uma semente, uma vez que queremos os mesmos dados\n# toda vez que rodamos esse c√≥digo. \nnp.random.seed(0)\ndados = random_weibull(n = n, alpha = alpha, beta = beta)\n\n# Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando\n# a fun√ß√£o log_likelihood.\nalpha, beta = np.round(opt.minimize(\n  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),\n  x0=[0.5, 0.5]\n).x, 2)\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\nprint(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\nprint(\"--> alpha: \", alpha, \"\\n\")\n\n--> alpha:  2.54 \n\nprint(\"--> beta: \", beta, \"\\n\")\n\n--> beta:  1.48"
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#conclus√µes",
    "href": "posts/likelihood_r_julia_python/index.html#conclus√µes",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Conclus√µes",
    "text": "Conclus√µes"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\nPython\n\n\nJulia\n\n\n \n\n\n\n\nSep 9, 2022\n\n\nProf.¬†Pedro Rafael D. Marinho\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about_pt_br.html",
    "href": "about_pt_br.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "‚¨ÜÔ∏è Meu ambiente de trabalho: Departamento de Estat√≠stica - UFPB"
  },
  {
    "objectID": "about_pt_br.html#sobre-mim",
    "href": "about_pt_br.html#sobre-mim",
    "title": "Prof.¬†Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOl√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\nForma√ß√£o\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014."
  },
  {
    "objectID": "about_en.html",
    "href": "about_en.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "Ol√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\n\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014.\n\n\n\n\n\n\n\n‚¨ÜÔ∏è Meu ambiente de trabalho: Departamento de Estat√≠stica - UFPB"
  },
  {
    "objectID": "about_en.html#sobre-mim",
    "href": "about_en.html#sobre-mim",
    "title": "Prof.¬†Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOl√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\nForma√ß√£o\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014."
  }
]