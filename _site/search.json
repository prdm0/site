[
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#uma-breve-introdu√ß√£o",
    "href": "posts/likelihood_r_julia_python/index.html#uma-breve-introdu√ß√£o",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Uma breve introdu√ß√£o",
    "text": "Uma breve introdu√ß√£o\nCi√™ncia de dados √©, sem d√∫vidas, uma √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.\n\n\n\n\n\n\nEu costumo dizer aos meus alunos que um √≥timo cientista de dados √© o profissional que sabe mais estat√≠stica que um cientista da computa√ß√£o mediano e mais computa√ß√£o de que um bom estat√≠stico mediano.\n\n\n\nTamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine! Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.\n\n\n\n\n\n\nDicas de linguagens\n\n\n\nSe voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando! Isso ir√° facilitar sua vida, pois voc√™ ter√° produtividad, aproveitando da infinidade de bibliotecas e frameworks dispon√≠veis para nossa √°rea.\nPor√©m, se mesmo assim voc√™ quiser insistir na pergunta üòü, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:\n\nR, veja R Core Team (2022)\nPython, veja Rossum (2010)\nJulia, veja Bezanson et al. (2017)\n\n\n\nSe voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados (bibliotecas e frameworks), i.e., voc√™ ter√° v√°rios presentinhos, gr√°tis, para poder utilizar nos seus projetos! üéÅ\nCada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal, sendo comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o, ok? üëç\nNo meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, (veja p.¬†74 de (Bezanson et al. 2017), quando surgiu a linguagem, em 2018 utilizei para construir c√≥digos de um artigo Marinho et al. (2018), mas somente a partir da pandemia de COVID-19 foi que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando mais recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas, estruturas de dados, fun√ß√µes varargs, closures, bibliotecas como NumPy e SciPy, pedras angulares para se fazer ci√™ncia de dados em Python, entre outros assuntos. Se voc√™ veio de R ou Julia, como foi o meu caso, e quer dominar a linguagem Python, estude a documenta√ß√£o oficial da linguagem que √© √≥tima. Procure conhecer:\n\nAs estruturas de dados de Python: arrays, tuplas, listas, dicion√°rios e conjuntos. Nesse t√≥pico, entenda que tuplas s√£o objetos imut√°veis, listas s√£o mut√°veis e dicion√°rios s√£o objetos mut√°veis, assim como as listas, por√©m possuem palavras-chave;\nEstude o conceito de fun√ß√µes varargs, t√©cnica muito poderosa, presente em todas √†s tr√™s linguagens, que permite generaliza√ß√µes que conduzem a c√≥digos muito mais √∫teis;\nEstude o conceito de closures, tamb√©m presente em todas √†s tr√™s linguagens que estamos conversando, e em diversas outras. Esse conceito permite que suas fun√ß√µes possam construir novas fun√ß√µes;\nSobretudo, experimente a linguagem!\n\n\n\n\n\n\n\nAs linguagens podem conversar entre si üéâ\n\n\n\nVoc√™ n√£o precisar√° apenas utilizar uma mesma linguagem de programa√ß√£o em um projeto de ci√™ncia de dados que esteja trabalhando. Se existe como fazer algo na linguagem que voc√™ est√° utilizando, fa√ßa nessa linguagem! Isso ir√° diminuir o uso de depend√™ncias. Por√©m, caso algo n√£o exista algo para sua linguagem e voc√™ n√£o est√° afim de implementar algum m√©todo do zero, ent√£o lembre-se que voc√™ poder√° importar c√≥digo de outras linguages.\nSe voc√™ est√°\n\nem R: voc√™ poder√° importar, utilizando a biblioteca reticulate, veja Ushey, Allaire, and Tang (2022), m√©todos e objetos com estruturas de dados em Python onde ser√£o convertidos para estruturas equivalentes em R. Caso queira importar c√≥digos Julia, voc√™ poder√° utilizar a biblioteca JuliaCall (Li 2019);\nem Julia: voc√™ poder√° importar c√≥digos R usando a biblioteca RCall. Para importar c√≥digos Python, voc√™ utilize a biblitoeca PyCall;\nem Python: voc√™ poder√° chamar c√≥digos R utilizando a biblioteca rpy2. J√° para chamar c√≥digos Julia, veja a biblioteca PyJulia.\n\n\n\nSe voc√™ j√° √© fluente em ao menos uma linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar outra(s) linguagem(ens) ser√° muito mais r√°pido. S√£o poucas semanas de estudo necess√°rias para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° üß† com a sintaxe das linguagens que voc√™ j√° programa. Normal!\nUm dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos Estimadores de M√°xima Verossimilhan√ßa - EMV. Quando estamos utilizando frameworks para ci√™ncia de dados, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.\nNa √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por ‚Äúbaixo dos panos‚Äù n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que precisar√° ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para obten√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward), aplicadas a problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo considerar√° os pesos sin√°pticos da arquitetura da rede com \\(n\\) conex√µes, isto √©, existir√° uma fun√ß√£o \\(f(y, w_1, w_2, \\cdots)\\), em fun√ß√£o dos pesos sin√°pticos \\(w_i, i = 1, ..., n\\) e da sa√≠da esperada. Como \\(y\\) √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontrar um conjunto √≥timo de valores \\(w_i\\) que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo backpropagation utiliza m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global de uma fun√ß√£o objetivo.\nSe voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de estat√≠stica computacional, na Se√ß√£o de Otimiza√ß√£o N√£o-linear.\n\n\n\n\n\n\nEm algum momento voc√™ tem que saber otimizar fun√ß√µes\n\n\n\nEm fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Escolher entre minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema em quest√£o.\nVoc√™, como um cientista de dados que √©, ou que almeja ser, ser√° o respons√°vel em descobrir se o que precisar√° fazer ser√° maximizar ou minimizar uma fun√ß√£o. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. üß†\nFun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üëç"
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#estimadores-de-m√°xima-verossimilhan√ßa---emv",
    "href": "posts/likelihood_r_julia_python/index.html#estimadores-de-m√°xima-verossimilhan√ßa---emv",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Estimadores de m√°xima verossimilhan√ßa - EMV",
    "text": "Estimadores de m√°xima verossimilhan√ßa - EMV\nPara n√£o ficarmos apenas olhado c√≥digos de programa√ß√£o em tr√™s linguagens distintas (R, Julia e Python), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da fun√ß√£o de verossimilhan√ßa. Serei muito breve! üéâ\n\n\n\n\n\n\nA maior barreira üß± de uma implementa√ß√£o consistente!\n\n\n\n\n\nA grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o, √© n√£o saber ao certo o que desejamos implementar.\n√â por isso que irei contextualizar, de forma breve, um problema comum na estat√≠stica e ci√™ncia de dados; o problema de otimizar uma fun√ß√£o objetivo. Mais precisamente, desejaremos obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.\n\n\n\nPara simplificar a teoria, irei considerar algumas premissas:\n\nVoc√™ tem algum conhecimento de probabilidade;\nConsiderarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por \\(X\\);\nA vari√°vel aleat√≥ria - v.a. \\(X\\) √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma fun√ß√£o densidade de probabilidade - fdp \\(f\\), tal que \\(f_X(x) \\geq 0\\) e \\(\\int_{-\\infty}^{+\\infty}f_X(x)\\, \\mathrm{d}x = 1\\).\n\nNa pr√°tica, o problema consiste em, por meio de um conjunto de dados, fixarmos uma fdp \\(f_X\\). Da√≠, desejaremos encontrar os par√¢metros de fdp que faz com que \\(f_X\\) melhor se ajuste aos dados. Os par√¢metros \\(\\alpha\\) e \\(\\beta\\) que far√° com que \\(f_X\\) melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa \\(\\mathcal{L}\\) de \\(f_X\\), em rela√ß√£o a \\(\\alpha\\) e \\(\\beta\\), definida por:\n\\[\n\\mathcal{L}(x, \\alpha,\\beta) = \\prod_{i = 1}^n f_{X_i}(x, \\alpha, \\beta).\n\\tag{1}\\] Para simplificar as coisas, como \\(\\log(\\cdot)\\) √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de \\(\\alpha\\) e \\(\\beta\\) que maximizam \\(\\mathcal{L}\\) ser√£o os mesmos que maximizam \\(\\log(\\mathcal{L})\\), ou seja, poderemos nos concentrar em maximizar:\n\\[\\ell(x, \\alpha, \\beta) = \\sum_{i = i}^n \\log[f_{X_i}(x, \\alpha, \\beta)]. \\tag{2}\\]\nSuponha que \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\), ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. \\(X\\) que tem observa√ß√µes que seguem a distribui√ß√£o \\(Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Essa ser√° nossa distribui√ß√£o verdadeira!\nNa pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥\nVamos supor que voc√™, assertivamente, escolhe a fam√≠lia Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma). Existem testes de ader√™ncias para checar o quanto uma distribui√ß√£o se ajusta a um conjunto de dados. N√£o entraremos nesse assunto aqui!\n\n\nQuer ver o c√≥digo do gr√°fico? Clique aqui!\nlibrary(ggplot2)\n\n# Quantidade de elementos\nn <- 550L\n# Par√¢metro de forma\nalpha <- 2.5\n# Par√¢metro de escala\nbeta <- 1.5\n\n# Fixando uma semente, de forma a sempre obtermos a mesma amostra\nset.seed(0)\ndados <- \n  data.frame(\n    x = seq(0, 2, length.out = n),\n    y_rand = rweibull(n, shape = alpha, scale = beta)\n  )\n\ndados |> \n  ggplot() +\n  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +\n  ggtitle(\n    label = \"Histograma do conjunto de dados\",\n    subtitle = \"Na pr√°tica voc√™ s√≥ tem eles\"\n  ) + \n  labs(\n    y = \"Densidade\",\n    x = \"x\"\n  ) + \n  scale_x_continuous(\n    limits = c(0, 2.7),\n    n.breaks = 15\n  ) + \n  scale_y_continuous(\n    limits = c(0, 1.05),\n    n.breaks = 15\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1),\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.1),\n    color = \"blue\",\n    size = 0.8\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.2),\n    color = \"tomato\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.3),\n    color = \"red\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = beta),\n    color = \"gold\",\n    size = 0.8\n  )\n\n\n\n\n\nNote que todas as fun√ß√µes densidades de probabilides - fdps plotadas no histograma apresentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de \\(\\alpha\\) e \\(\\beta\\), respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequada. Precisamos escolher (estimar) adequadamente os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.\nLembre-se, para obter essas estimativas, temos que maximizar a Equation¬†1, ou equivalentemente a Equation¬†2.\nNo gr√°fico acima, √© poss√≠vel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. N√£o iremos fazer testes de adequa√ß√£o!\n\nDe fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de \\(X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)\\). Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de \\(X\\). Segue logo abaixo:\n\\[f_X(x) = (\\alpha/\\beta)(x/\\beta)^{\\alpha - 1}\\exp[{-(x/\\beta)^\\alpha}],\\] com \\(x\\), \\(\\alpha\\) e \\(\\beta > 0\\).\n\nImplementa√ß√µes\nAgora que j√° conhecemos \\(f_X\\) e \\(\\ell(\\cdot)\\) (‚Äúfun√ß√£o objetivo‚Äù), poderemos colocar as ‚Äúm√£os na massa‚Äù no teclado.\n\n\n\n\n\n\nE os dados?\n\n\n\nOs dados ser√£o gerados aleatoriamente, em cada uma das linguagens (R, Julia e Python). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° das implementa√ß√µes dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a \\(\\alpha = 2.5\\) e \\(\\beta = 1.5\\), nas tr√™s linguagens. Por isso, n√£o irie comparar os resultados das estimativas obtidas, a n√£o ser que viesse a ter resultados muito incoerentes.\nIrei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles. Nada em excesso!\n\n\nAntes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?\n\nNumpy: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£o de n√∫meros pseudo aleat√≥rios, entre outras coisas. Consulte mais detalhes em https://numpy.org/doc/stable;\nScipy: trata-se de outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos e algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integra√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em https://scipy.org.\n\nIremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o Weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html.\nNo link voc√™ ver√° que o que √© implementado pelo m√©todo random.weibull √© gerar observa√ß√µes de \\(X = [-\\log(U)]^{1/\\alpha} \\sim Weibull(\\alpha)\\), com \\(U\\) sendo um v.a. uniforme no intervalo (0,1] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o \\(Weibull(\\alpha, \\beta)\\), teremos que multiplicar o resultado de random.weibull pelo valor de \\(\\beta\\). Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da \\(Weibull(\\alpha, \\beta)\\).\n\n\n\nEu olhando param um pouco de malabarismo de c√≥digo (desnecess√°rio) em Python.\n\n\nVeja como seria em R, Julia e Python:\n\nRJuliaPython\n\n\n\nset.seed(0)\nrweibull(n = 10L, shape = 2.5, scale = 1.5)\n\n [1] 0.6181884 1.6792787 1.4930932 1.1870595 0.5881789 1.8107341 0.6138897\n [8] 0.4766274 1.0544363 1.1027820\n\n\n\n\n\nusing Distributions\nusing Random\n\nRandom.seed!(0);\nrand(Weibull(2.5,1.5), 10)\n\n10-element Vector{Float64}:\n 1.3361401397866541\n 1.0507258710483653\n 0.8178793280596004\n 0.6700247875189858\n 0.6429703222489156\n 0.6315072585920245\n 2.313206178975804\n 2.048795844959291\n 1.1773995533033512\n 1.6047832712314394\n\n\n\n\n\nimport numpy as np\n\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha, n)\n\nnp.random.seed(0)\n\nrandom_weibull(n = 10, alpha = 2.5, beta = 1.5)\n\narray([1.36908085, 1.64315124, 1.4528271 , 1.36309319, 1.18186313,\n       1.52263868, 1.20258335, 2.06494277, 2.42259084, 1.12172534])\n\n\n\n\n\n\n\n\n\n\n\nIsso √© um pouco estranho, mas tudo bem, sabemos programar!\n\n\n\nTer que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o random_weibull, em que foi preciso considerar beta * np.random.weibull(alpha, n) para se ter observa√ß√µes Weibull com valores de \\(\\beta\\) diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.\nAfinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que \\(\\beta = 1\\), como ocorre em random.weibull ele poderia especificar isso como argumento passados √† fun√ß√£o, certo?\nComportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de prop√≥sito geral, assim como Python √© em sua ess√™ncia.\nNote que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo random.weibull e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as novidades de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.\n\n\nAgora, sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.\n\nNas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamento de c√≥digo:\n\nNote que utilizo o conceito de fun√ß√µes com argumentos vari√°veis, tamb√©m chamadas de fun√ß√µes varargs. Perceba que a fun√ß√£o log_likelihood n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de log_likelihood nos tr√™s c√≥digos (R, Python e Julia). Precisamos de fun√ß√µes com operadores varargs, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos varargs √© uma t√©cnica muito poderosa. Utilizei esses conceitos em Python e Julia, devido a natureza das fun√ß√µes de otimiza√ß√µes das duas linguagens, em que em seus designers permitem que os argumentos a serem otimizados da fun√ß√£o objetivo seja de n√∫mero vari√°vel. As fun√ß√µes varargs de R s√£o definidas pelo operador dot-dot-dot (...). O designer da fun√ß√£o optim() de R incluem o par√¢metro dot-dot-dot, para argumentos extras que eventualmente possam existir na fun√ß√£o objetivo. ‚ö°\nNote que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para \\(\\alpha\\) e \\(\\beta\\) . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se ad√©que a diversas outras situa√ß√µes!\nOutro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecidas como fun√ß√µes lambda.\nFoi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo BFGS. Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno - BFGS pertencem, nos materiais que disponibilizo aos meus alunos na disciplina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes a respeito dos m√©todos de Newtone quasi-Newton, clique aqui.\n\n\n\n\n\n\n\nMinimizar ou maximizar?\n\n\n\nAlguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, \\(-f\\) equivale a maximizar \\(f\\), em que \\(f\\) √© uma dada fun√ß√£o objetivo.\n\n\n\nRJuliaPython\n\n\n\n# Quantidade de observa√ß√µes\nn <- 250L\n\n# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o\n# da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes.\nalpha <- 2.5\nbeta <- 1.5\n\n# Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios.\n# Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir \n# os mesmos dados.\nset.seed(3)\n\n# Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para \n# modelar.\ndados <- rweibull(n = n, shape = alpha, scale = beta)\n\npdf_weibull <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n}\n\n# Checando se a densidade de pdf_weibull integra em 1\narea = integrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))\n\n# Em R, o operador dot-dot-dot (...) √© utilizado para definir\n# quantidade vari√°dica de argumentos. Assim, log_likelihood √©\n# uma fun√ß√£o vararg.\nlog_likelihood <- function(x, pdf, par)\n  -sum(log(pdf(x, par)))\n\nresult <- optim(\n  fn = log_likelihood,\n  par = c(0.5, 0.5),\n  method = \"BFGS\",\n  x = dados,\n  pdf = pdf_weibull\n)\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\ncat(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\ncat(\"--> alpha: \", result$par[1], \"\\n\")\n\n--> alpha:  2.86476 \n\ncat(\"--> beta: \", result$par[2], \"\\n\")\n\n--> beta:  1.503992 \n\n\n\n\n\nusing Distributions\nusing Random\nusing Optim\nusing QuadGK\n\n# Quantidade de observa√ß√µes\nn = 250;\n\n# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., \n# da fdp da v.a. cujos os dados s√£o observa√ß√µes.\nŒ± = 2.5;\nŒ≤ = 1.5;\n\n# Fixando um valor de semente\nRandom.seed!(0);\n\n# Gerando um array de dados com distribui√ß√£o Weibull(Œ±, Œ≤)\ndados = rand(Weibull(Œ±,Œ≤), n);\n\n# Fun√ß√£o densidade de probaiblidade de uma v.a. \n# X ‚àº Weibull(Œ±, Œ≤)\nfunction pdf_weibull(x, par = (Œ±, Œ≤))\n    Œ±, Œ≤ = par.Œ±, par.Œ≤   \n    @. Œ±/Œ≤ * (x/Œ≤)^(Œ±-1) * exp(-(x/Œ≤)^Œ±)\nend;\n\n# Checando se a integral no suporte de pdf_weibull integra em 1\narea, error = quadgk(x -> pdf_weibull(x, (Œ± = Œ±, Œ≤ = Œ≤)), 0, Inf);\n\n# Escrevendo a fun√ß√£o log_likelihood que em julia denotarei por\n# ‚Ñì, tendo em vista que podemos fazer uso de caracteres UTF-8 \n# nessa linguagem. \nfunction ‚Ñì(x, pdf, par...)\n    -sum(log.(pdf(x, par...)))\nend;\n\n# Encontrando as estimativas de m√°xima verossimilhan√ßa usando a\n# biblioteca Optim\nemv = optimize(\n        x -> ‚Ñì(dados, pdf_weibull, (Œ± = x[1], Œ≤ = x[2])),\n        [0.5, 0.5],\n        LBFGS()\n); \n\nemv_Œ±, emv_Œ≤  = emv.minimizer;\n\n# Imprimindo o resultado\n\nprint(\"Valores estimados para Œ± e Œ≤\\n\")\n\nValores estimados para Œ± e Œ≤\n\nprint(\"--> Œ±: \", emv_Œ±, \"\\n\")\n\n--> Œ±: 2.4472234393032837\n\nprint(\"--> Œ≤: \", emv_Œ≤, \"\\n\")\n\n--> Œ≤: 1.4480344422787308\n\n\n\n\n\nimport numpy as np\nimport scipy.stats as stat\nimport scipy.integrate as inte\nimport scipy.optimize as opt\n\n# Valores da distribui√ß√£o verdadeira\nalpha = 2.5\nbeta = 1.5\n\n# N√∫mero de observa√ß√µes que ir√£o compor nossos dados\nn = 250 \n\n# Implementando a fun√ß√£o random_weibull, em que os par√¢metros\n# que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais\n# sentido ser assim, n√£o?\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha,n)\n\n# Escrevendo a fun√ß√£p densidade de probabilidade da Weibull\n# na reparametriza√ß√£o correta.\ndef pdf_weibull(x, param):\n  alpha = param[0]\n  beta = param[1]\n  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)\n\n# Testando se a densidade integra em 1\n√°rea = round(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),\n      0, np.inf, args = (1,1))[0],2)\n\n# Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo\n# (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o \n# ir√° receber como argumento uma fun√ß√£o densidade de probabilidade.\n# N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de \n# log-verossimilhan√ßa!\n# A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1\n# devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar \n# uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f.\n# Lembre-se disso!\ndef log_likelihood(x, pdf, *args):\n  return -np.sum(np.log(pdf(np.array(x), *args)))\n\n# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa \n# √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera\n# que gerou os dados que desejamos ajustar.\n# Precisamos fixar uma semente, uma vez que queremos os mesmos dados\n# toda vez que rodamos esse c√≥digo. \nnp.random.seed(0)\ndados = random_weibull(n = n, alpha = alpha, beta = beta)\n\n# Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando\n# a fun√ß√£o log_likelihood.\nalpha, beta = opt.minimize(\n  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),\n  x0=[0.5, 0.5]\n).x\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\nprint(\"Valores estimados de alpha e beta\\n\")\n\nValores estimados de alpha e beta\n\nprint(\"--> alpha: \", alpha, \"\\n\")\n\n--> alpha:  2.541634329050904 \n\nprint(\"--> beta: \", beta, \"\\n\")\n\n--> beta:  1.480430581806816 \n\n\n\n\n\n\n\nVisualiza√ß√£o gr√°fica\n\n\nQuer ver o c√≥digo do gr√°fico? Clique aqui!\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nQuer ver o c√≥digo do gr√°fico? Clique aqui!\nlibrary(latex2exp)\n\nset.seed(0)\n\ndados <- rweibull(n = 500L, shape = 2.5, scale = 1.5)\n\npdf_weibull <- function(x, alpha, beta)\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n\n# log-verossimilhan√ßa\nlog_likelihood <- function(x, alpha, beta)\n  prod(pdf_weibull(x, alpha, beta))\n\nvec_log_likelihood <- \n  Vectorize(\n    FUN = log_likelihood,\n    vectorize.args = c(\"alpha\", \"beta\")\n  )\n\nalpha <- seq(2.4, 2.87, length.out = length(dados))\nbeta <- seq(1.42, 1.56, length.out = length(dados))\ndf_contour <- expand_grid(alpha, beta)\n\ndf_contour <- \n  df_contour |> \n  mutate(z = vec_log_likelihood(x = dados, alpha, beta))\n  \ndf_contour |> \n  ggplot() + \n  geom_contour_filled(aes(x = alpha, y = beta, z = z)) +\n  ggtitle(\n    label = \"Curvas de n√≠veis da fu√ß√£o de Verossimilhan√ßa\", \n  ) +\n  xlab(TeX(r'(\\alpha)')) +\n  ylab(TeX(r'(\\beta)')) +\n  labs(fill = \"N√≠veis\") +\n  theme(\n    plot.title  = element_text(face = \"bold\"),\n    legend.title = element_text(face = \"bold\")\n  ) +\n  # R\n  geom_point(\n    x = 2.86476,\n    y = 1.503992,\n    color = \"red\",\n    size = 2\n  )"
  },
  {
    "objectID": "posts/likelihood_r_julia_python/index.html#conclus√µes",
    "href": "posts/likelihood_r_julia_python/index.html#conclus√µes",
    "title": "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python",
    "section": "Conclus√µes",
    "text": "Conclus√µes\nEssa postagem conseguiu exemplificar como poderemos implementar a solu√ß√£o de um problema de estima√ß√£o por m√°xima verossimilhan√ßa, utilizando as linguagens de programa√ß√£o R, Python e Julia. Nas tr√™s linguagens, abordamos conceitos interessantes como fun√ß√µes varargs e fun√ß√µes an√¥nimas, al√©m de algumas estruturas de dados e bibliotecas.\nForam observados √≥timas estimativas, muito embora elas n√£o s√£o compar√°veis, tendo em vista que os conjuntos de dados gerados aleatoriamente foram distintos, por conta da natureza de implementa√ß√£o das fun√ß√µes para gera√ß√£o de n√∫meros pseudo-aleat√≥rios, dispon√≠veis em cada uma das linguagens comparadas. Foi escolhido fazer dessa forma, para que fosse poss√≠vel abordar o problema de gera√ß√£o de n√∫meros pseudo-aleat√≥rios em cada linguagem.\nTamb√©m √© poss√≠vel perceber que otimizar uma fun√ß√£o objetivo √© f√°cil, independentemente da linguagem, sendo poss√≠vel chegar a c√≥digos concisos e eficientes.\nAt√© o pr√≥ximo post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nR\n\n\nPython\n\n\nJulia\n\n\n \n\n\n\n\nSep 9, 2022\n\n\nProf.¬†Pedro Rafael D. Marinho\n\n\n19 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about_pt_br.html",
    "href": "about_pt_br.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "‚¨ÜÔ∏è Meu ambiente de trabalho: Departamento de Estat√≠stica - UFPB"
  },
  {
    "objectID": "about_pt_br.html#sobre-mim",
    "href": "about_pt_br.html#sobre-mim",
    "title": "Prof.¬†Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOl√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\nForma√ß√£o\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014."
  },
  {
    "objectID": "about_en.html",
    "href": "about_en.html",
    "title": "Prof.¬†Pedro Rafael",
    "section": "",
    "text": "Ol√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\n\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014.\n\n\n\n\n\n\n\n‚¨ÜÔ∏è Meu ambiente de trabalho: Departamento de Estat√≠stica - UFPB"
  },
  {
    "objectID": "about_en.html#sobre-mim",
    "href": "about_en.html#sobre-mim",
    "title": "Prof.¬†Pedro Rafael",
    "section": "Sobre mim",
    "text": "Sobre mim\nOl√°, meu nome √© Pedro Rafael D. Marinho e sou Professor do Departamento de Estat√≠stica da Universidade Federal da Para√≠ba - UFPB.\n\nSou entusiasta das linguagens de programa√ß√£o R e Julia, por√©m, tamb√©m me interesso por outas linguagens.\nAtualmente, tenho interesses na aplica√ß√£o e no desenvolvimento de bibliotecas computacionais nas √°reas de ci√™ncia de dados e aprendizagem de m√°quina. Uma das coisas mais nobres na ci√™ncia √© desenvolver ferramentas cient√≠ficas que possam ser amplamente utilizadas na produ√ß√£o e aplica√ß√£o da ci√™ncia.\n\nForma√ß√£o\n\nBacharel em estat√≠stica pela Universidade Federal da Para√≠ba - UFPB, 2010;\nMestre em estat√≠stica pela Universidade Federal de Pernambuco, 2012;\nDoutor em estat√≠stica pela Universidade Federal de Pernambuco, 2014."
  }
]