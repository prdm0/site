{
  "hash": "60a3353dfeec5f90bb6e78855812f5b0",
  "result": {
    "markdown": "---\ntitle: \"Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python\"\nauthor: \"Prof. Pedro Rafael D. Marinho\"\ndate: \"2022-09-09\"\ncategories: [news, code, R, Python, Julia]\n---\n\n\n![](imagem_animacao.gif)\n\n## Uma breve introdu√ß√£o\n\n[Ci√™ncia de dados](https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados) √©, sem d√∫vidas, um √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.\n\n::: {.callout-note appearance=\"simple\"}\nEu coscumo dizer aos meus alunos que um **√≥timo cientista de dados** √© o profissional que sabe mais estat√≠stica que um bom cientista da computa√ß√£o e mais computa√ß√£o de que um bom estat√≠stico.\n:::\n\nTamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine. Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.\n\n::: callout-note\n## Dicas de linguagens\n\nSe voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando.\n\nPor√©m, se mesmo assim voc√™ quiser insistir na pergunta, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:\n\n1.  [**R**](https://www.r-project.org)\n2.  [**Python**](https://www.python.org)\n3.  [**Julia**](https://julialang.org)\n:::\n\nSe voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados.\n\nCada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal e √© comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o!\n\nNo meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, quando surgiu a linguagem, mas somente a partir da pandemia de COVID-19 que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando muito recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas (fun√ß√µes varargs), closures, bibliotecas como [NumPy](https://numpy.org) e [SciPy](https://scipy.org), que s√£o pedras angulares para se fazer ci√™ncia de dados em Python.\n\nSe voc√™ j√° √© fluente em ao menos um linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar uma outra linguagem √© muito mais r√°pido. S√£o poucas semanas para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° com a sintaxe das linguagens que voc√™ j√° programa.\n\nUm dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos **Estimadores de M√°xima Verossimilhan√ßa** - **EMV**. Quando estamos utilizando frameworks de machine learning, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.\n\nNa √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por \"baixo dos panos\" n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que percisa ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para oben√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward) aplicadas √† problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo que levar√° em considera√ß√£o os pesos sin√°pticos da arquitetura da rede com $n$ conex√µes, isto √©, existir√° uma fun√ß√£o $f(y, w_1, w_2, \\cdots)$, em fun√ß√£o dos pesos sin√°piticos $w_i, i = 1, ..., n$ e da saida esperada. Como $y$ √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontar um conjunto √≥timo de valores $w_i$ que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo backpropagation que tamb√©m faz uso de m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global da fun√ß√£o objetivo.\n\nSe voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de [**estat√≠stica computacional**](https://prdm0.github.io/aulas_computacional/), no Se√ß√£o de [**Otimiza√ß√£o N√£o-linear**](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).\n\n::: callout-tip\n## Em algum momento voc√™ tem que saber otimizar fun√ß√µes\n\nEm fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema.\n\nVoc√™, como um cientista de dados que √© ou que almeja ser √© que dever√° saber se o que precisa √© maximizar ou minimizar. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. ‚úåÔ∏è\n\nFun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üòÖ\n:::\n\n## Estimadores de m√°xima verossimilhan√ßa - EMV\n\nPara que n√£o fiquemos apenas olhado c√≥digos de programa√ß√£o em tr√™s linguagens distintas (**R**, **Julia** e **Python**), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da [**fun√ß√£o de verossimilhan√ßa**](https://en.wikipedia.org/wiki/Likelihood_function). Serei muito breve. üéâ\n\n::: {.callout-caution collapse=\"true\"}\n## A maior barreira üß± de uma implementa√ß√£o consistente!\n\nA grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o √© n√£o saber ao certo o que desejamos implementar.\n\n√â por isso que irei contextualizar, de forma breve, um comum na estat√≠stica e ci√™ncia de dados que √© o problema de otimizar uma fun√ß√£o objetivo, mais precisamente, obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.\n:::\n\nPara simplificar a teoria, irei considerar algumas premissas:\n\n1.  Voc√™ tem algum conhecimento de probabilidade;\n\n2.  Considerarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por $X$;\n\n3.  A vari√°vel aleat√≥ria - v.a. $X$ √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma [fun√ß√£o densidade de probabilidade - fdp](https://en.wikipedia.org/wiki/Probability_density_function) $f$, tal que $f_X(x) \\geq 0$ e $\\int_{-\\infty}^{+\\infty}f_X(x)\\, \\mathrm{d}x = 1$.\n\nNa pr√°tica, o problema consiste em, atrav√©s de um conjunto de dados, fixarmos uma fdp $f_X$. Da√≠, desejamos encontrar os par√¢metros de fdp que faz com que $f_X$ melhor se ajuste aos dados. Os par√¢metros $\\alpha$ e $\\beta$ que far√° com que $f_X$ melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa $\\mathcal{L}$ de $f_X$, em rela√ß√£o a $\\alpha$ e $\\beta$, definida por:\n\n$$\n\\mathcal{L}(x, \\alpha,\\beta) = \\prod_{i = 1}^n f_{X_i}(x, \\alpha, \\beta).\n$$ {#eq-objetivo} Para simplificar as coisas, como $\\log(\\cdot)$ √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de $\\alpha$ e $\\beta$ que maximizam $\\mathcal{L}$ ser√£o os mesmos que maximizam $\\log(\\mathcal{L})$, ou seja, poderemos nos concentrar em maximizar:\n\n$$\\ell(x, \\alpha, \\beta) = \\sum_{i = i}^n \\log[f_{X_i}(x, \\alpha, \\beta)].$$ {#eq-logobjetivo}\n\nSuponha que $X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)$, ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. $X$ que tem observa√ß√µes que segue a distribui√ß√£o $Weibull(\\alpha = 2.5, \\beta = 1.5)$. **Essa ser√° nossa distribui√ß√£o verdadeira!**\n\nNa pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥\n\nVamos supor que voc√™, acertivamente, escolhe a fam√≠la Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Quer ver o c√≥digo do gr√°fico? Clique aqui!\"}\nlibrary(ggplot2)\n\n# Quantidade de elementos\nn <- 550L\n# Par√¢metro de forma\nalpha <- 2.5\n# Par√¢metro de escala\nbeta <- 1.5\n\n# Fixando uma semente, de forma a sempre obtermos a mesma amostra\nset.seed(0)\ndados <- \n  data.frame(\n    x = seq(0, 2, length.out = n),\n    y_rand = rweibull(n, shape = alpha, scale = beta)\n  )\n\ndados |> \n  ggplot() +\n  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +\n  ggtitle(\n    label = \"Histograma do conjunto de dados\",\n    subtitle = \"Na pr√°tica voc√™ s√≥ tem eles\"\n  ) + \n  labs(\n    y = \"Densidade\",\n    x = \"x\"\n  ) + \n  scale_x_continuous(\n    limits = c(0, 2.7),\n    n.breaks = 15\n  ) + \n  scale_y_continuous(\n    limits = c(0, 1.05),\n    n.breaks = 15\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1),\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.1),\n    color = \"blue\",\n    size = 0.8\n  ) +\n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.2),\n    color = \"tomato\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = 1.3),\n    color = \"red\",\n    size = 0.8\n  ) + \n  geom_function(\n    fun = dweibull,\n    args = list(shape = alpha, scale = beta),\n    color = \"gold\",\n    size = 0.8\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nNote que todas as fun√ß√µes densidades de probabilidas - fdps plotadas no histograma aprensentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de $\\alpha$ e $\\beta$, respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequadade. Precisamos escolher (estimar) adequadade os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.\n\nLembre-se, para obter essas estimativas, temos que maximizar a @eq-objetivo, ou equivalentemente a @eq-logobjetivo.\n\nNo gr√°fico acima, √© possivel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. De fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de $X \\sim Weibull(\\alpha = 2.5, \\beta = 1.5)$. Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de $X$. Segue logo abaixo:\n\n$$f_X(x) = (\\alpha/\\beta)(x/\\beta)^{\\alpha - 1}\\exp[{-(x/\\beta)^\\alpha}],$$ com $x$, $\\alpha$ e $\\beta > 0$.\n\n### Implementa√ß√µes\n\nAgora que j√° conhecemos $f_X$ e $\\ell(\\cdot)$ (\"fun√ß√£o objetivo\"), poderemos colocar as \"~~m√£os na massa~~\" no teclado.\n\n::: callout-tip\n# E os dados?\n\nOs dados ser√£o gerados aleatoriamente, em cada uma das linguagem ([**R**](https://www.r-project.org), [**Julia**](https://julialang.org) e [**Python**](https://www.python.org)). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° da implementa√ß√£o dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a $\\alpha = 2.5$ e $\\beta = 1.5$, nas tr√™s linguagens.\n\nIrei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles.\n:::\n\nAntes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?\n\n1.  **Numpy**: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£om de n√∫meros pseudo atelat√≥rios, entre outras coisas. Consulte mais detalhes em <https://numpy.org/doc/stable>;\n\n2.  **Scipy**: tarta-se de uma outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos de algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integrama√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em <https://scipy.org>.\n\nIremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificiamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em <https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html>.\n\nNo [link](https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html) voc√™ ver√° que o que √© implementado pelo m√©todo `random.weibull` √© gerar observa√ß√µes de $X = [-\\log(U)]^{1/\\alpha} \\sim Weibull(\\alpha)$, com $U$ sendo um v.a. uniforme no intervalo (0,1\\] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o $Weibull(\\alpha, \\beta)$, teremos que multiplicar o resultado de `random.weibull` pelo valor de $\\beta$. Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da $Weibull(\\alpha, \\beta)$.\n\nVeja como seria em R, Julia e Python:\n\n::: panel-tabset\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(0)\nrweibull(n = 10L, shape = 2.5, scale = 1.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.6181884 1.6792787 1.4930932 1.1870595 0.5881789 1.8107341 0.6138897\n [8] 0.4766274 1.0544363 1.1027820\n```\n:::\n:::\n\n\n## Julia\n\n\n::: {.cell}\n\n```{.julia .cell-code}\nusing Distributions\nusing Random\n\nRandom.seed!(0);\nrand(Weibull(2.5,1.5), 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n10-element Vector{Float64}:\n 1.3361401397866541\n 1.0507258710483653\n 0.8178793280596004\n 0.6700247875189858\n 0.6429703222489156\n 0.6315072585920245\n 2.313206178975804\n 2.048795844959291\n 1.1773995533033512\n 1.6047832712314394\n```\n:::\n:::\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha, n)\n\nnp.random.seed(0)\n\nrandom_weibull(n = 10, alpha = 2.5, beta = 1.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([1.36908085, 1.64315124, 1.4528271 , 1.36309319, 1.18186313,\n       1.52263868, 1.20258335, 2.06494277, 2.42259084, 1.12172534])\n```\n:::\n:::\n\n:::\n\n![Eu olhando param um pouco de malabarismo de c√≥digo (desnecess√°rio) em Python.](nao_entendi.gif)\n\n::: callout-tip\n# Isso √© um pouco estranho, mas tudo bem, sabemos programar!\n\nTer que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o `random_weibull`, em que foi preciso considerar `beta * np.random.weibull(alpha, n)` para se ter observa√ß√µes Weibull com valores de $\\beta$ diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.\n\nAfinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que $\\beta = 1$, como ocorre em `random.weibull` ele poderia especificar isso como argumento da fun√ß√£o, certo?\n\nComportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de propr√≥sito geral, assim como Python √© em sua ess√™ncia.\n\nNote que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo `random.weibull` e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as [novidades](https://docs.python.org/3.11/whatsnew/3.11.html) de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.\n:::\n\nAgora sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.\n\n![](preguica_feliz.gif)\n\nNas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamente de c√≥digo:\n\n1.  Note que utilizo o conceito de fun√ß√µes com argumentos vari√°dicos, tamb√©m chamadas de fun√ß√µes varargs. Perceba que a fun√ß√£o `log_likelihood` n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de `log_likelihood` nos tr√™s c√≥digos (R, Python e Julia). Precisamos de operadores **varargs**, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos **varargs** √© uma t√©cnica muito poderosa. ‚ö°\n\n2.  Note que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para $\\alpha$ e $\\beta$ . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se adeque a diversas outras situa√ß√µes!\n\n3.  Outro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecida como [**fun√ß√µes lambda](https://en.wikipedia.org/wiki/Anonymous_function)**.\n\n4.  Foi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de **B**royden--**F**letcher--**G**oldfarb--**S**hanno - **BFGS** pertencem, nos materiais que diponibilizo aos meus alunos na discipina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes, clique [aqui](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).\n\n::: callout-tip\n# Minimizar ou maximizar?\n\nAlguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, $-f$ equivale a maximizar $f$, em que $f$ √© uma dada fun√ß√£o objetivo.\n:::\n\n::: panel-tabset\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# Quantidade de observa√ß√µes\nn <- 250L\n\n# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o\n# da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes.\nalpha <- 2.5\nbeta <- 1.5\n\n# Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios.\n# Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir \n# os mesmos dados.\nset.seed(3)\n\n# Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para \n# modelar.\ndados <- rweibull(n = n, shape = alpha, scale = beta)\n\npdf_weibull <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)\n}\n\n# Checando se a densidade de pdf_weibull integra em 1\nintegrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1 with absolute error < 6e-06\n```\n:::\n\n```{.r .cell-code}\n# Em R, o operador dot-dot-dot (...) √© utilizado para definir\n# quantidade vari√°dica de argumentos. Assim, log_likelihood √©\n# uma fun√ß√£o vararg.\nlog_likelihood <- function(x, pdf, par)\n  -sum(log(pdf(x, par)))\n\nresult <- optim(\n  fn = log_likelihood,\n  par = c(0.5, 0.5),\n  method = \"BFGS\",\n  x = dados,\n  pdf = pdf_weibull\n)\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\ncat(\"Valores estimados de alpha e beta\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores estimados de alpha e beta\n```\n:::\n\n```{.r .cell-code}\ncat(\"--> alpha: \", result$par[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--> alpha:  2.86476 \n```\n:::\n\n```{.r .cell-code}\ncat(\"--> beta: \", result$par[2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--> beta:  1.503992 \n```\n:::\n:::\n\n\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport scipy.stats as stat\nimport scipy.integrate as inte\nimport scipy.optimize as opt\n\n# Valores da distribui√ß√£o verdadeira\nalpha = 2.5\nbeta = 1.5\n\n# N√∫mero de observa√ß√µes que ir√£o compor nossos dados\nn = 250 \n\n# Implementando a fun√ß√£o random_weibull, em que os par√¢metros\n# que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais\n# sentido ser assim, n√£o?\ndef random_weibull(n, alpha, beta):\n  return beta * np.random.weibull(alpha,n)\n\n# Escrevendo a fun√ß√£p densidade de probabilidade da Weibull\n# na reparametriza√ß√£o correta.\ndef pdf_weibull(x, param):\n  alpha = param[0]\n  beta = param[1]\n  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)\n\n# Testando se a densidade integra em 1\nround(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),\n      0, np.inf, args = (1,1))[0],2)\n\n# Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo\n# (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o \n# ir√° receber como argumento uma fun√ß√£o densidade de probabilidade.\n# N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de \n# log-verossimilhan√ßa!\n# A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1\n# devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar \n# uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f.\n# Lembre-se disso!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.0\n```\n:::\n\n```{.python .cell-code}\ndef log_likelihood(x, pdf, *args):\n  return -np.sum(np.log(pdf(np.array(x), *args)))\n\n# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa \n# √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera\n# que gerou os dados que desejamos ajustar.\n# Precisamos fixar uma semente, uma vez que queremos os mesmos dados\n# toda vez que rodamos esse c√≥digo. \nnp.random.seed(0)\ndados = random_weibull(n = n, alpha = alpha, beta = beta)\n\n# Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando\n# a fun√ß√£o log_likelihood.\nalpha, beta = np.round(opt.minimize(\n  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),\n  x0=[0.5, 0.5]\n).x, 2)\n\n# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa\nprint(\"Valores estimados de alpha e beta\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nValores estimados de alpha e beta\n```\n:::\n\n```{.python .cell-code}\nprint(\"--> alpha: \", alpha, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--> alpha:  2.54 \n```\n:::\n\n```{.python .cell-code}\nprint(\"--> beta: \", beta, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--> beta:  1.48 \n```\n:::\n:::\n\n:::\n\n## Conclus√µes\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}