---
title: "Estima√ß√£o por m√°xima verossimilhan√ßa em R, Julia e Python"
author: "Prof. Pedro Rafael D. Marinho"
date: "2022-09-09"
categories: [news, code, R, Python, Julia]
bibliography: references.bib
---

![](r_julia_python_animacao.gif)

## Uma breve introdu√ß√£o

[Ci√™ncia de dados](https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados) √©, sem d√∫vidas, uma √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.

::: {.callout-note appearance="simple"}
Eu costumo dizer aos meus alunos que um **√≥timo cientista de dados** √© o profissional que sabe mais estat√≠stica que um cientista da computa√ß√£o mediano e mais computa√ß√£o de que um bom estat√≠stico mediano.
:::

Tamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine! Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.

::: callout-note
## Dicas de linguagens

Se voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando! Isso ir√° facilitar sua vida, pois voc√™ ter√° produtividade, aproveitando da infinidade de bibliotecas e frameworks dispon√≠veis para nossa √°rea.

Por√©m, se mesmo assim voc√™ quiser insistir na pergunta üòü, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:

1.  [**R**](https://www.r-project.org), veja @linguagemr
2.  [**Python**](https://www.python.org), veja @vanrossum2010
3.  [**Julia**](https://julialang.org), veja @bezanson2017
:::

Se voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados (bibliotecas e frameworks), i.e., voc√™ ter√° v√°rios presentinhos, gr√°tis, para poder utilizar nos seus projetos! üéÅ

Cada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal, sendo comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o, ok? üëç

No meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, (veja p. 74 de [@bezanson2017], quando surgiu a linguagem, em 2018 utilizei para construir c√≥digos de um artigo @marinho2018, mas somente a partir da pandemia de COVID-19 foi que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando mais recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas, estruturas de dados, fun√ß√µes varargs, closures, bibliotecas como [NumPy](https://numpy.org) e [SciPy](https://scipy.org), pedras angulares para se fazer ci√™ncia de dados em Python, entre outros assuntos. Se voc√™ veio de R ou Julia, como foi o meu caso, e quer dominar a linguagem Python, estude a [documenta√ß√£o oficial](https://docs.python.org/pt-br/3/tutorial/index.html) da linguagem que √© √≥tima. Procure conhecer:

1.  As **estruturas de dados** de Python: arrays, tuplas, listas, dicion√°rios e conjuntos. Nesse t√≥pico, entenda que tuplas s√£o objetos imut√°veis, listas s√£o mut√°veis e dicion√°rios s√£o objetos mut√°veis, assim como as listas, por√©m possuem palavras-chave;

2.  Estude o conceito de **fun√ß√µes varargs**, t√©cnica muito poderosa, presente em todas √†s tr√™s linguagens, que permite generaliza√ß√µes que conduzem a c√≥digos muito mais √∫teis;

3.  Estude o **conceito de closures**, tamb√©m presente em todas √†s tr√™s linguagens que estamos conversando, e em diversas outras. Esse conceito permite que suas fun√ß√µes possam construir novas fun√ß√µes;

4.  Sobretudo, experimente a linguagem!

::: callout-tip
## As linguagens podem conversar entre si üéâ

Voc√™ n√£o precisar√° apenas utilizar uma mesma linguagem de programa√ß√£o em um projeto de ci√™ncia de dados que esteja trabalhando. Se existe como fazer algo na linguagem que voc√™ est√° utilizando, fa√ßa nessa linguagem! Isso ir√° diminuir o uso de depend√™ncias. Por√©m, caso algo n√£o exista para sua linguagem e voc√™ n√£o est√° afim de implementar algum m√©todo do zero, ent√£o lembre-se que voc√™ poder√° importar c√≥digo de outras linguagens.

Se voc√™ est√°

1.  **em R**: voc√™ poder√° importar, utilizando a biblioteca **reticulate**, veja @reticulate, m√©todos e objetos com estruturas de dados em Python onde ser√£o convertidos para estruturas equivalentes em R. Caso queira importar c√≥digos Julia, voc√™ poder√° utilizar a biblioteca **JuliaCall** [@JuliaCall];

2.  **em Julia**: voc√™ poder√° importar c√≥digos R usando a biblioteca [**RCall**](https://juliainterop.github.io/RCall.jl/stable/). Para importar c√≥digos Python, voc√™ dever√° utilizar a biblioteca [**PyCall**](https://github.com/JuliaPy/PyCall.jl);

3.  **em Python**: voc√™ poder√° chamar c√≥digos R utilizando a biblioteca [**rpy2**](https://rpy2.github.io/doc/latest/html/introduction.html). J√° para chamar c√≥digos Julia, veja a biblioteca [**PyJulia**](https://github.com/JuliaPy/pyjulia).
:::

Se voc√™ j√° √© fluente em ao menos uma linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar outra(s) linguagem(ens) ser√° muito mais r√°pido. S√£o poucas semanas de estudo necess√°rias para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° üß† com a sintaxe das linguagens que voc√™ j√° programa. Normal!

Um dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos **Estimadores de M√°xima Verossimilhan√ßa** - **EMV**. Quando estamos utilizando frameworks para ci√™ncia de dados, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.

Na √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por "baixo dos panos" n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que precisar√° ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para obten√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward), aplicadas a problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo que considerar√° os pesos sin√°pticos da arquitetura da rede com $n$ conex√µes, isto √©, existir√° uma fun√ß√£o $f(y, w_1, w_2, \cdots)$, em fun√ß√£o dos pesos sin√°pticos $w_i, i = 1, ..., n$ e da sa√≠da esperada. Como $y$ √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontrar um conjunto √≥timo de valores $w_i$ que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) utiliza m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global de uma fun√ß√£o objetivo.

Se voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de [**estat√≠stica computacional**](https://prdm0.github.io/aulas_computacional/), na Se√ß√£o de [**Otimiza√ß√£o N√£o-linear**](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).

::: callout-tip
## Em algum momento voc√™ tem que saber otimizar fun√ß√µes

Em fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Escolher entre minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema em quest√£o.

Voc√™, como um cientista de dados que √©, ou que almeja ser, ser√° o respons√°vel em descobrir se o que precisar√° fazer ser√° maximizar ou minimizar uma fun√ß√£o. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. üß†

Fun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üëç
:::

## Estimadores de m√°xima verossimilhan√ßa - EMV

Para n√£o ficarmos apenas olhando c√≥digos de programa√ß√£o em tr√™s linguagens distintas (**R**, **Julia** e **Python**), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da [**fun√ß√£o de verossimilhan√ßa**](https://en.wikipedia.org/wiki/Likelihood_function). Serei muito breve! üéâ

::: {.callout-caution collapse="true"}
## A maior barreira üß± de uma implementa√ß√£o consistente!

A grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o, √© n√£o saber ao certo o que desejamos implementar.

√â por isso que irei contextualizar, de forma breve, um problema comum na estat√≠stica e ci√™ncia de dados; o problema de otimizar uma fun√ß√£o objetivo. Mais precisamente, desejaremos obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.
:::

Para simplificar a teoria, irei considerar algumas premissas:

1.  Voc√™ tem algum conhecimento de probabilidade;

2.  Considerarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por $X$;

3.  A vari√°vel aleat√≥ria - v.a. $X$ √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma [fun√ß√£o densidade de probabilidade - fdp](https://en.wikipedia.org/wiki/Probability_density_function) $f$, tal que $f_X(x) \geq 0$ e $\int_{-\infty}^{+\infty}f_X(x)\, \mathrm{d}x = 1$.

Na pr√°tica, o problema consiste em, por meio de um conjunto de dados, fixarmos uma fdp $f_X$. Da√≠, desejaremos encontrar os par√¢metros de fdp que faz com que $f_X$ melhor se ajuste aos dados. Os par√¢metros $\alpha$ e $\beta$ que far√° com que $f_X$ melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa $\mathcal{L}$ de $f_X$, em rela√ß√£o a $\alpha$ e $\beta$, definida por:

$$
\mathcal{L}(x, \alpha,\beta) = \prod_{i = 1}^n f_{X_i}(x, \alpha, \beta).
$$ {#eq-objetivo} Para simplificar as coisas, como $\log(\cdot)$ √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de $\alpha$ e $\beta$ que maximizam $\mathcal{L}$ ser√£o os mesmos que maximizam $\log(\mathcal{L})$, ou seja, poderemos nos concentrar em maximizar:

$$\ell(x, \alpha, \beta) = \sum_{i = i}^n \log[f_{X_i}(x, \alpha, \beta)].$$ {#eq-logobjetivo}

Suponha que $X \sim Weibull(\alpha = 2.5, \beta = 1.5)$, ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. $X$ que tem observa√ß√µes que seguem a distribui√ß√£o $Weibull(\alpha = 2.5, \beta = 1.5)$. **Essa ser√° nossa distribui√ß√£o verdadeira!**

Na pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥

Vamos supor que voc√™, assertivamente, escolhe a fam√≠lia Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma). Existem testes de ader√™ncias para checar o quanto uma distribui√ß√£o se ajusta a um conjunto de dados. N√£o entraremos nesse assunto aqui!

```{r, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "Quer ver o c√≥digo do gr√°fico? Clique aqui!"
library(ggplot2)

# Quantidade de elementos
n <- 550L
# Par√¢metro de forma
alpha <- 2.5
# Par√¢metro de escala
beta <- 1.5

# Fixando uma semente, de forma a sempre obtermos a mesma amostra
set.seed(0)
dados <- 
  data.frame(
    x = seq(0, 2, length.out = n),
    y_rand = rweibull(n, shape = alpha, scale = beta)
  )

dados |> 
  ggplot() +
  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +
  ggtitle(
    label = "Histograma do conjunto de dados",
    subtitle = "Na pr√°tica voc√™ s√≥ tem eles"
  ) + 
  labs(
    y = "Densidade",
    x = "x"
  ) + 
  scale_x_continuous(
    limits = c(0, 2.7),
    n.breaks = 15
  ) + 
  scale_y_continuous(
    limits = c(0, 1.05),
    n.breaks = 15
  ) +
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1),
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.1),
    color = "blue",
    size = 0.8
  ) +
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.2),
    color = "tomato",
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.3),
    color = "red",
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = beta),
    color = "gold",
    size = 0.8
  )
```

Note que todas as fun√ß√µes densidades de probabilidades - fdps plotadas no histograma apresentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de $\alpha$ e $\beta$, respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequada. Precisamos escolher (estimar) adequadamente os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.

Lembre-se, para obter essas estimativas, temos que maximizar a @eq-objetivo, ou equivalentemente a @eq-logobjetivo.

No gr√°fico acima, √© poss√≠vel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. N√£o iremos fazer testes de adequa√ß√£o!\
\
De fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de $X \sim Weibull(\alpha = 2.5, \beta = 1.5)$. Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de $X$. Segue logo abaixo:

$$f_X(x) = (\alpha/\beta)(x/\beta)^{\alpha - 1}\exp[{-(x/\beta)^\alpha}],$$ com $x$, $\alpha$ e $\beta > 0$.

### Implementa√ß√µes

Agora que j√° conhecemos $f_X$ e $\ell(\cdot)$ ("fun√ß√£o objetivo"), poderemos colocar as "~~m√£os na massa~~" no teclado.

::: callout-tip
# E os dados?

Os dados ser√£o gerados aleatoriamente, em cada uma das linguagens ([**R**](https://www.r-project.org), [**Julia**](https://julialang.org) e [**Python**](https://www.python.org)). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° das implementa√ß√µes dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a $\alpha = 2.5$ e $\beta = 1.5$, nas tr√™s linguagens. Por isso, n√£o irie comparar, inicialmente, os resultados das estimativas obtidas. Ao final da postagem, farei uma sucinta compara√ß√£o.

Irei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles. Nada em excesso!
:::

Antes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?

1.  **Numpy**: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£o de n√∫meros pseudo-aleat√≥rios, entre outras coisas. Consulte mais detalhes em <https://numpy.org/doc/stable>;

2.  **Scipy**: trata-se de outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos e algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integra√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em <https://scipy.org>.

Iremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o Weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em <https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html>.

No [link](https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html) voc√™ ver√° que o que √© implementado pelo m√©todo `random.weibull` √© gerar observa√ß√µes de $X = [-\log(U)]^{1/\alpha} \sim Weibull(\alpha)$, com $U$ sendo um v.a. uniforme no intervalo (0,1\] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o $Weibull(\alpha, \beta)$, teremos que multiplicar o resultado de `random.weibull` pelo valor de $\beta$. Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da $Weibull(\alpha, \beta)$.

![Eu olhando para um pouco de malabarismo de c√≥digo em Python.](nao_entendi.gif)

Veja como seria em R, Julia e Python:

::: panel-tabset
## R

```{r}
set.seed(0)
rweibull(n = 10L, shape = 2.5, scale = 1.5)
```

## Julia

```{julia}
using Distributions
using Random

Random.seed!(0);
rand(Weibull(2.5,1.5), 10)
```

## Python

```{python}
import numpy as np

def random_weibull(n, alpha, beta):
  return beta * np.random.weibull(alpha, n)

np.random.seed(0)

random_weibull(n = 10, alpha = 2.5, beta = 1.5)
```
:::

::: callout-tip
# Isso √© um pouco estranho, mas tudo bem, sabemos programar!

Ter que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o `random_weibull`, em que foi preciso considerar `beta * np.random.weibull(alpha, n)` para se ter observa√ß√µes Weibull com valores de $\beta$ diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.

Afinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que $\beta = 1$, como ocorre em `random.weibull` ele poderia especificar isso como argumento passados √† fun√ß√£o, certo?

Comportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de prop√≥sito geral, assim como Python √© em sua ess√™ncia.

Note que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo `random.weibull` e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as [novidades](https://docs.python.org/3.11/whatsnew/3.11.html) de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.
:::

Agora, sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.

![](preguica_feliz.gif)

Nas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamento de c√≥digo:

1.  Note que utilizo o conceito de fun√ß√µes com argumentos vari√°veis, tamb√©m chamadas de **fun√ß√µes varargs**. Perceba que a fun√ß√£o `log_likelihood` n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de `log_likelihood` nos tr√™s c√≥digos (R, Python e Julia). Precisamos de fun√ß√µes com operadores **varargs**, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos **varargs** √© uma t√©cnica muito poderosa. Utilizei esses conceitos em Python e Julia, devido a natureza das fun√ß√µes de otimiza√ß√µes das duas linguagens, em que em seus designers permitem que os argumentos a serem otimizados da fun√ß√£o objetivo seja de n√∫mero vari√°vel. As fun√ß√µes varargs de R s√£o definidas pelo operador dot-dot-dot (`...`). O designer da fun√ß√£o `optim()` de R incluem o par√¢metro dot-dot-dot, para argumentos extras que eventualmente possam existir na fun√ß√£o objetivo. ‚ö°

2.  Note que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para $\alpha$ e $\beta$ . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se ad√©que a diversas outras situa√ß√µes!

3.  Outro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecidas como [**fun√ß√µes lambda**](https://en.wikipedia.org/wiki/Anonymous_function).

4.  Foi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de **B**royden--**F**letcher--**G**oldfarb--**S**hanno - **BFGS** pertencem, nos materiais que disponibilizo aos meus alunos na disciplina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes a respeito dos m√©todos de Newtone quasi-Newton, clique [aqui](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).

::: callout-tip
# Minimizar ou maximizar?

Alguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, $-f$ equivale a maximizar $f$, em que $f$ √© uma dada fun√ß√£o objetivo.
:::

::: panel-tabset
## R

```{r}
# Quantidade de observa√ß√µes
n <- 250L

# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o
# da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes.
alpha <- 2.5
beta <- 1.5

# Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios.
# Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir 
# os mesmos dados.
set.seed(0)

# Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para 
# modelar.
dados <- rweibull(n = n, shape = alpha, scale = beta)

pdf_weibull <- function(x, par){
  alpha <- par[1]
  beta <- par[2]
  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)
}

# Checando se a densidade de pdf_weibull integra em 1
area = integrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))

# Em R, o operador dot-dot-dot (...) √© utilizado para definir
# quantidade vari√°dica de argumentos. Assim, log_likelihood √©
# uma fun√ß√£o vararg.
log_likelihood <- function(x, pdf, par)
  -sum(log(pdf(x, par)))

result <- optim(
  fn = log_likelihood,
  par = c(0.5, 0.5),
  method = "BFGS",
  x = dados,
  pdf = pdf_weibull
)

# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa
cat("Valores estimados de alpha e beta\n")
cat("--> alpha: ", result$par[1], "\n")
cat("--> beta: ", result$par[2], "\n")
```

## Julia

```{julia}
using Distributions
using Random
using Optim
using QuadGK

# Quantidade de observa√ß√µes
n = 250;

# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., 
# da fdp da v.a. cujos os dados s√£o observa√ß√µes.
Œ± = 2.5;
Œ≤ = 1.5;

# Fixando um valor de semente
Random.seed!(0);

# Gerando um array de dados com distribui√ß√£o Weibull(Œ±, Œ≤)
dados = rand(Weibull(Œ±,Œ≤), n);

# Fun√ß√£o densidade de probaiblidade de uma v.a. 
# X ‚àº Weibull(Œ±, Œ≤)
function pdf_weibull(x, par = (Œ±, Œ≤))
    Œ±, Œ≤ = par.Œ±, par.Œ≤   
    @. Œ±/Œ≤ * (x/Œ≤)^(Œ±-1) * exp(-(x/Œ≤)^Œ±)
end;

# Checando se a integral no suporte de pdf_weibull integra em 1
area, error = quadgk(x -> pdf_weibull(x, (Œ± = Œ±, Œ≤ = Œ≤)), 0, Inf);

# Escrevendo a fun√ß√£o log_likelihood que em julia denotarei por
# ‚Ñì, tendo em vista que podemos fazer uso de caracteres UTF-8 
# nessa linguagem. 
function ‚Ñì(x, pdf, par...)
    -sum(log.(pdf(x, par...)))
end;

# Encontrando as estimativas de m√°xima verossimilhan√ßa usando a
# biblioteca Optim
emv = optimize(
        x -> ‚Ñì(dados, pdf_weibull, (Œ± = x[1], Œ≤ = x[2])),
        [0.5, 0.5],
        LBFGS()
); 

emv_Œ±, emv_Œ≤  = emv.minimizer;

# Imprimindo o resultado

print("Valores estimados para Œ± e Œ≤\n")
print("--> Œ±: ", emv_Œ±, "\n")
print("--> Œ≤: ", emv_Œ≤, "\n")
```

## Python

```{python}
import numpy as np
import scipy.stats as stat
import scipy.integrate as inte
import scipy.optimize as opt

# Valores da distribui√ß√£o verdadeira
alpha = 2.5
beta = 1.5

# N√∫mero de observa√ß√µes que ir√£o compor nossos dados
n = 250 

# Implementando a fun√ß√£o random_weibull, em que os par√¢metros
# que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais
# sentido ser assim, n√£o?
def random_weibull(n, alpha, beta):
  return beta * np.random.weibull(alpha,n)

# Escrevendo a fun√ß√£p densidade de probabilidade da Weibull
# na reparametriza√ß√£o correta.
def pdf_weibull(x, param):
  alpha = param[0]
  beta = param[1]
  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)

# Testando se a densidade integra em 1
√°rea = round(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),
      0, np.inf, args = (1,1))[0],2)

# Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo
# (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o 
# ir√° receber como argumento uma fun√ß√£o densidade de probabilidade.
# N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de 
# log-verossimilhan√ßa!
# A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1
# devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar 
# uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f.
# Lembre-se disso!
def log_likelihood(x, pdf, *args):
  return -np.sum(np.log(pdf(np.array(x), *args)))

# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa 
# √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera
# que gerou os dados que desejamos ajustar.
# Precisamos fixar uma semente, uma vez que queremos os mesmos dados
# toda vez que rodamos esse c√≥digo. 
np.random.seed(0)
dados = random_weibull(n = n, alpha = alpha, beta = beta)

# Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando
# a fun√ß√£o log_likelihood.
alpha, beta = opt.minimize(
  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),
  x0=[0.5, 0.5]
).x

# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa
print("Valores estimados de alpha e beta\n")
print("--> alpha: ", alpha, "\n")
print("--> beta: ", beta, "\n")
```
:::

### Visualiza√ß√£o gr√°fica

Vamos agorar comparar graficamente as estimativas obtidas pelos m√©todos de otimiza√ß√£o (BFGS) considerando as bibliotecas fornecidas nas tr√™s linguages. Para uma melhor visualiza√ß√£o, utilizaremos o gr√°fico de curvas de n√≠veis. Perceba que $\mathcal{L}$ possui como vari√°veis os par√¢metros $\alpha$ e $\beta$, sendo $x$ o conjunto de dados que √© fixo. Assim, consiguiremos visualizar em 2D o comportamento de $\mathcal{L}$.

Para que possamos visualizar graficamente e comparar as estimativas obtidas nas tr√™s linguagens, ser√° preciso que o conjunto de dados seja id√™ntico nos processos de otimiza√ß√£o realizados em R, Julia e Python.

Para acessar o conjunto de dados, clique [aqui](dados.csv). O conjunto de dados possui $n = 500$ observa√ß√µes. Ainda continuarei considerando $\alpha = 2.5$ e $\beta = 1.5$.

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "Quer ver o c√≥digo do gr√°fico? Clique aqui!"
#| warning: false
library(ggplot2)
library(tidyr)
library(dplyr)
library(latex2exp)

set.seed(0)

dados <- rweibull(n = 500L, shape = 2.5, scale = 1.5)

pdf_weibull <- function(x, alpha, beta)
  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)

# log-verossimilhan√ßa
log_likelihood <- function(x, alpha, beta)
  prod(pdf_weibull(x, alpha, beta))

vec_log_likelihood <- 
  Vectorize(
    FUN = log_likelihood,
    vectorize.args = c("alpha", "beta")
  )

alpha <- seq(2.4, 2.87, length.out = length(dados))
beta <- seq(1.42, 1.56, length.out = length(dados))
df_contour <- expand_grid(alpha, beta)

df_contour <- 
  df_contour |> 
  mutate(z = vec_log_likelihood(x = dados, alpha, beta))
  
df_contour |> 
  ggplot() + 
  geom_contour_filled(aes(x = alpha, y = beta, z = z), show.legend = FALSE) +
  ggtitle(
    label = "Curvas de n√≠veis da fu√ß√£o de Verossimilhan√ßa", 
    subtitle = "Comparativo R, Python e Julia" 
  ) +
  xlab(TeX(r'(\alpha)')) +
  ylab(TeX(r'(\beta)')) +
  labs(fill = "N√≠veis") +
  theme(
    plot.title  = element_text(face = "bold"),
    legend.title = element_text(face = "bold")
  ) +
  # R
  geom_point(
    x = 2.64276,
    y = 1.488277 ,
    color = "blue",
    size = 3
  ) +
  # Python
  geom_point(
    x = 2.642850353244295,
    y = 1.488271661049081,
    color = "green",
    size = 2.5
  ) + 
  # Julia
  geom_point(
    x = 2.642850346755736,
    y = 1.4882716649411558 ,
    color = "red",
    size = 2
  )

```

Perceba que foi obtido √≥timas estimativas pelo m√©todo BFGS, nas tr√™s linguagens. Note que tive que colocar um ponto maior que o outro, para que eles n√£o ficassem totalmente sobrepostos.

## Conclus√µes

Espero que esse post tenha conseguido exemplificar como poderemos implementar a solu√ß√£o de um problema de estima√ß√£o por m√°xima verossimilhan√ßa, utilizando as linguagens de programa√ß√£o R, Python e Julia. Abordamos conceitos interessantes como fun√ß√µes varargs e fun√ß√µes an√¥nimas, al√©m de algumas estruturas de dados e bibliotecas.

Nos primeiros c√≥digos, foram observados √≥timas estimativas, muito embora elas n√£o s√£o compar√°veis, tendo em vista que os conjuntos de dados gerados aleatoriamente foram distintos, por conta da natureza de implementa√ß√£o das fun√ß√µes para gera√ß√£o de n√∫meros pseudo-aleat√≥rios, dispon√≠veis em cada uma das linguagens comparadas. Foi escolhido fazer dessa forma, para que fosse poss√≠vel abordar o problema de gera√ß√£o de n√∫meros pseudo-aleat√≥rios em cada linguagem.

No fim, foi realizado uma compara√ß√£o mais justa, onde o mesmo conjunto de dados foi considerado para a produ√ß√£o das curvas de n√≠veis, √∫teis para visualizar em uma imagem 2D a qualidade das estimativas. No gr√°fico de curvas de n√≠veis, √© poss√≠vel observar a sobreposi√ß√£o das estimativas obtidas utilizando R, Python e Julia.

Tamb√©m √© poss√≠vel perceber que otimizar uma fun√ß√£o objetivo √© f√°cil, independentemente da linguagem, sendo poss√≠vel conseguir c√≥digos concisos e eficientes.

Todos os gr√°ficos dessa postagem foram constru√≠dos em R, atrav√©s da biblioteca @ggplot2. Caso queira construir gr√°ficos em Python ou em Julia, considere as seguintes cito a biblioteca [seaborn](https://seaborn.pydata.org) de Python e a biblioteca [Makie](https://docs.makie.org/stable/) de Julia. S√£o apenas sugest√µes, tendo em vista que h√° diversas outras alternativas interessantes.

## Refer√™ncias
