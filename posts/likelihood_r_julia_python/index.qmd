---
title: "Estima√ß√£o por m√°xima verossimilha√ßa em R, Julia e Python"
author: "Prof. Pedro Rafael D. Marinho"
date: "2022-09-09"
categories: [news, code, R, Python, Julia]
---

![](imagem_animacao.gif)

## Uma breve introdu√ß√£o

[Ci√™ncia de dados](https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados) √©, sem d√∫vidas, um √°rea de pesquisa que permeia diversas outras ci√™ncias, estando mais intimamente relacionada com as √°reas de estat√≠stica e a computa√ß√£o.

::: {.callout-note appearance="simple"}
Eu coscumo dizer aos meus alunos que um **√≥timo cientista de dados** √© o profissional que sabe mais estat√≠stica que um bom cientista da computa√ß√£o e mais computa√ß√£o de que um bom estat√≠stico.
:::

Tamb√©m √© importante dizer, antes de irmos ao t√≥pico desse post, que √© poss√≠vel fazer ci√™ncia de dados em qualquer linguagem de programa√ß√£o que voc√™ domine. Claro, isso n√£o implica que a produ√ß√£o de ci√™ncia de dados √© igualmente f√°cil em qualquer linguagem que voc√™ escolha.

::: callout-note
## Dicas de linguagens

Se voc√™ me permitir te dar uma dica de qual linguagem de programa√ß√£o escolher, sem citar nomes, eu pediria para que voc√™ se aproximasse das linguagens que a comunidade que faz ci√™ncia de dados est√£o utilizando.

Por√©m, se mesmo assim voc√™ quiser insistir na pergunta, me tirando da regi√£o de conforto de neutralidade, eu citaria tr√™s linguagens para voc√™ escolher:

1.  [**R**](https://www.r-project.org)
2.  [**Python**](https://www.python.org)
3.  [**Julia**](https://julialang.org)
:::

Se voc√™ est√° estudando algumas dessas linguagens ou domina ao menos uma delas, certamente voc√™ estar√° tra√ßando um caminho congruente e ter√° a sua disposi√ß√£o um arsenal de ferramentas prontas para trabalhar com ci√™ncia de dados.

Cada cientista de dados, por √≥bvio, tem sua hist√≥ria pessoal e √© comum trilharem caminhos diferentes na programa√ß√£o. N√£o se faz ci√™ncia de dados sem programa√ß√£o!

No meu caso, programo em R a muito mais tempo que em Julia e Python, algo acima de uma d√©cada. A linguagem Julia, tive o primeiro contato em 2012, quando surgiu a linguagem, mas somente a partir da pandemia de COVID-19 que comecei a estudar os manuais da linguagem com um pouco mais de seriedade. A linguagem Python venho estudando muito recentemente, por√©m, j√° consigo conversar sobre temas como fun√ß√µes vari√°dicas (fun√ß√µes varargs), closures, bibliotecas como [NumPy](https://numpy.org) e [SciPy](https://scipy.org), que s√£o pedras angulares para se fazer ci√™ncia de dados em Python.

Se voc√™ j√° √© fluente em ao menos um linguagem de programa√ß√£o, nos concentraremos nessas tr√™s citadas acima, ent√£o perceber√° que dominar uma outra linguagem √© muito mais r√°pido. S√£o poucas semanas para que voc√™ j√° consiga produzir c√≥digos com boas pr√°ticas de programa√ß√£o, coisa que foi muito mais √°rduo no aprendizado da primeira linguagem. Claro, vez ou outra voc√™ se pegar√° olhando com const√¢ncia os manuais dessa(s) nova(s) linguagem(ens), pois a sintaxe √© nova para voc√™ e eventualmente se misturar√° com a sintaxe das linguagens que voc√™ j√° programa.

Um dos problemas corriqueiros para quem trabalha com ci√™ncia de dados e, em particular, com infer√™ncia estat√≠stica √© a obten√ß√£o dos **Estimadores de M√°xima Verossimilhan√ßa** - **EMV**. Quando estamos utilizando frameworks de machine learning, muitas vezes essas estima√ß√µes ocorrem por baixo do pano. H√° diversas situa√ß√µes em que estimadores com boas propriedades estat√≠sticas precisam ser utilizados, e os EMV s√£o, de longe, os mais utilizados.

Na √°rea de machine learning, por exemplo, se o que est√° sendo otimizado por "baixo dos panos" n√£o √© uma fun√ß√£o de log-verossimilhan√ßa, existir√° alguma fun√ß√£o objetivo que percisa ser maximizada ou minimizada, utilizando m√©todos de otimiza√ß√µes globais; m√©todos de Newton e quasi-Newton, os mesmos que utilizaremos para oben√ß√£o das estimativas obtidas pelos EMV, aqui nessa postagem. Seguindo com outro exemplo, na √°rea de redes neurais alimentadas para frente (feedforward) aplicadas √† problemas de regress√£o ou classifica√ß√£o, existe uma fun√ß√£o objetivo que levar√° em considera√ß√£o os pesos sin√°pticos da arquitetura da rede com $n$ conex√µes, isto √©, existir√° uma fun√ß√£o $f(y, w_1, w_2, \cdots)$, em fun√ß√£o dos pesos sin√°piticos $w_i, i = 1, ..., n$ e da saida esperada. Como $y$ √© conhecido (sa√≠da esperada da rede), para que a rede esteja bem ajustada, ser√° preciso encontar um conjunto √≥timo de valores $w_i$ que minimize essa fun√ß√£o. Isso √© feito pelo algoritmo backpropagation que tamb√©m faz uso de m√©todos de otimiza√ß√£o n√£o-linear para encontrar o m√≠nimo global da fun√ß√£o objetivo.

Se voc√™ quer conhecer mais profundamente essas metodologias de otimiza√ß√£o, escrevi sobre elas no meu material de [**estat√≠stica computacional**](https://prdm0.github.io/aulas_computacional/), no Se√ß√£o de [**Otimiza√ß√£o N√£o-linear**](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).

::: callout-tip
## Em algum momento voc√™ tem que saber otimizar fun√ß√µes

Em fim, voc√™ em algum momento, no seu percurso na √°rea de ci√™ncia de dados, ir√° ter que otimizar fun√ß√µes. Quando digo otimizar, em geral, me refiro a maximizar ou minimizar uma fun√ß√£o objetivo. Minimizar ‚¨áÔ∏è ou maximizar ‚¨ÜÔ∏è depender√° da natureza do problema.

Voc√™, como um cientista de dados que √© ou que almeja ser √© que dever√° saber se o que precisa √© maximizar ou minimizar. Sobretudo, voc√™ que precisar√° saber qual fun√ß√£o dever√° otimizar! Isso vai al√©m da programa√ß√£o. Portanto, procure sempre entender o problema e conhecer os detalhes das metodologias que deseja utilizar. ‚úåÔ∏è

Fun√ß√µes objetivos ir√£o sempre aparecer na sua vida! üòÖ
:::

## Estimadores de m√°xima verossimilhan√ßa - EMV

Para que n√£o fiquemos apenas olhado c√≥digos de programa√ß√£o em tr√™s linguagens distintas (**R**, **Julia** e **Python**), irei contextualizar um simples problema: o problema de encontrar o m√°ximo da [**fun√ß√£o de verossimilhan√ßa**](https://en.wikipedia.org/wiki/Likelihood_function). Serei muito breve. üéâ

::: {.callout-caution collapse="true"}
## A maior barreira üß± de uma implementa√ß√£o consistente!

A grande barreira que limita a nossa implementa√ß√£o, quando j√° dominamos ao menos uma linguagem de programa√ß√£o √© n√£o saber ao certo o que desejamos implementar.

√â por isso que irei contextualizar, de forma breve, um comum na estat√≠stica e ci√™ncia de dados que √© o problema de otimizar uma fun√ß√£o objetivo, mais precisamente, obter o m√°ximo da fun√ß√£o de verossimilhan√ßa.
:::

Para simplificar a teoria, irei considerar algumas premissas:

1.  Voc√™ tem algum conhecimento de probabilidade;

2.  Considerarei o caso univariado, em que teremos uma √∫nica vari√°vel aleat√≥ria - v.a. que denotarei por $X$;

3.  A vari√°vel aleat√≥ria - v.a. $X$ √© cont√≠nua, portanto suas observa√ß√µes podem ser modeladas por uma [fun√ß√£o densidade de probabilidade - fdp](https://en.wikipedia.org/wiki/Probability_density_function) $f$, tal que $f_X(x) \geq 0$ e $\int_{-\infty}^{+\infty}f_X(x)\, \mathrm{d}x = 1$.

Na pr√°tica, o problema consiste em, atrav√©s de um conjunto de dados, fixarmos uma fdp $f_X$. Da√≠, desejamos encontrar os par√¢metros de fdp que faz com que $f_X$ melhor se ajuste aos dados. Os par√¢metros $\alpha$ e $\beta$ que far√° com que $f_X$ melhor se ajuste aos dados poder√£o ser obtidos maximizando a fun√ß√£o de verossimilhan√ßa $\mathcal{L}$ de $f_X$, em rela√ß√£o a $\alpha$ e $\beta$, definida por:

$$
\mathcal{L}(x, \alpha,\beta) = \prod_{i = 1}^n f_{X_i}(x, \alpha, \beta).
$$ {#eq-objetivo} Para simplificar as coisas, como $\log(\cdot)$ √© uma fun√ß√£o mon√≥tona, ent√£o, os valores de $\alpha$ e $\beta$ que maximizam $\mathcal{L}$ ser√£o os mesmos que maximizam $\log(\mathcal{L})$, ou seja, poderemos nos concentrar em maximizar:

$$\ell(x, \alpha, \beta) = \sum_{i = i}^n \log[f_{X_i}(x, \alpha, \beta)].$$ {#eq-logobjetivo}

Suponha que $X \sim Weibull(\alpha = 2.5, \beta = 1.5)$, ou seja, que os dados que chegam a sua mesa s√£o provenientes de uma v.a. $X$ que tem observa√ß√µes que segue a distribui√ß√£o $Weibull(\alpha = 2.5, \beta = 1.5)$. **Essa ser√° nossa distribui√ß√£o verdadeira!**

Na pr√°tica, voc√™ apenas conhecer√° os dados! Ser√° voc√™, como cientista de dados, que ir√° supor alguma fam√≠lia de distribui√ß√µes para modelar os dados em quest√£o. ü•¥

Vamos supor que voc√™, acertivamente, escolhe a fam√≠la Weibull de distribui√ß√µes para modelar o seu conjunto de dados (voc√™ far√° isso olhando o comportamento dos dados, por exemplo, fazendo um histograma).

```{r, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "Quer ver o c√≥digo do gr√°fico? Clique aqui!"
library(ggplot2)

# Quantidade de elementos
n <- 550L
# Par√¢metro de forma
alpha <- 2.5
# Par√¢metro de escala
beta <- 1.5

# Fixando uma semente, de forma a sempre obtermos a mesma amostra
set.seed(0)
dados <- 
  data.frame(
    x = seq(0, 2, length.out = n),
    y_rand = rweibull(n, shape = alpha, scale = beta)
  )

dados |> 
  ggplot() +
  geom_histogram(aes(x = y_rand, y = ..density..), bins = 15) +
  ggtitle(
    label = "Histograma do conjunto de dados",
    subtitle = "Na pr√°tica voc√™ s√≥ tem eles"
  ) + 
  labs(
    y = "Densidade",
    x = "x"
  ) + 
  scale_x_continuous(
    limits = c(0, 2.7),
    n.breaks = 15
  ) + 
  scale_y_continuous(
    limits = c(0, 1.05),
    n.breaks = 15
  ) +
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1),
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.1),
    color = "blue",
    size = 0.8
  ) +
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.2),
    color = "tomato",
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = 1.3),
    color = "red",
    size = 0.8
  ) + 
  geom_function(
    fun = dweibull,
    args = list(shape = alpha, scale = beta),
    color = "gold",
    size = 0.8
  )
```

Note que todas as fun√ß√µes densidades de probabilidas - fdps plotadas no histograma aprensentado no gr√°fico acima s√£o densidades da fam√≠lia Weibull de distribui√ß√µes. O que difere uma da outra s√£o os valores de $\alpha$ e $\beta$, respectivamente. N√£o basta escolher uma fam√≠lia de distribui√ß√µes adequadade. Precisamos escolher (estimar) adequadade os par√¢metros que indexam a distribui√ß√£o, sendo o m√©todo de m√°xima verossimilhan√ßa, a metodologia estat√≠stica que nos ajudam a fazer uma √≥tima escolha, conduzindo as estimativas que s√£o provenientes de estimadores com boas propriedades estat√≠sticas.

Lembre-se, para obter essas estimativas, temos que maximizar a @eq-objetivo, ou equivalentemente a @eq-logobjetivo.

No gr√°fico acima, √© possivel visualmente perceber que a curva em amarelo √© a que melhor aproxima o comportamento dos dados. De fato, essa √© a curva da distribui√ß√£o verdadeira, i.e., √© a curva da fdp de $X \sim Weibull(\alpha = 2.5, \beta = 1.5)$. Por sinal, ainda n√£o coloquei a equa√ß√£o da fdp de $X$. Segue logo abaixo:

$$f_X(x) = (\alpha/\beta)(x/\beta)^{\alpha - 1}\exp[{-(x/\beta)^\alpha}],$$ com $x$, $\alpha$ e $\beta > 0$.

### Implementa√ß√µes

Agora que j√° conhecemos $f_X$ e $\ell(\cdot)$ ("fun√ß√£o objetivo"), poderemos colocar as "~~m√£os na massa~~" no teclado.

::: callout-tip
# E os dados?

Os dados ser√£o gerados aleatoriamente, em cada uma das linguagem ([**R**](https://www.r-project.org), [**Julia**](https://julialang.org) e [**Python**](https://www.python.org)). Sendo assim, muito provavelmente n√£o ser√£o os mesmos dados, em cada linguagem, pois a sequ√™ncia gerada que corresponder√° aos nossos dados depender√° da implementa√ß√£o dos geradores de n√∫meros pseudo-aleat√≥rios de cada linguagem. Por√©m, os resultados das estimativas devem convergir para valores pr√≥ximos a $\alpha = 2.5$ e $\beta = 1.5$, nas tr√™s linguagens.

Irei colocar coment√°rios nos c√≥digos para que voc√™ possa estudar cada um deles.
:::

Antes irei colocar uma observa√ß√£o para a linguagem Python. As pedras angulares para computa√ß√£o cient√≠fica em Python s√£o as bibliotecas NumPy e Scipy. Por que elas s√£o √∫teis?

1.  **Numpy**: √© uma biblioteca de c√≥digo aberto iniciada em 2005 e que possui diversos m√©todos (fun√ß√µes) num√©ricas comumente utilizadas na computa√ß√£o cient√≠fica. H√° diversos m√©todos para operar sobre arrays, vetoriza√ß√£o, gera√ß√£om de n√∫meros pseudo atelat√≥rios, entre outras coisas. Consulte mais detalhes em <https://numpy.org/doc/stable>;

2.  **Scipy**: tarta-se de uma outra biblioteca importante que cont√©m implementa√ß√µes de m√©todos de algoritmos fundamentais para computa√ß√£o cient√≠fica, como m√©todos de integrama√ß√£o, interpola√ß√£o, otimiza√ß√£o, entre diversas outras metodologias. Consulte outros detalhes em <https://scipy.org>.

Iremos utilizar ambas as bibliotecas. Basicamente a Numpy ser√° utilizada para vetoriza√ß√£o de c√≥digo, trabalhar com arrays e gerar observa√ß√µes da distribui√ß√£o Weibull. Nesse √∫ltimo ponto, especificiamente, a biblioteca Numpy implementa a fun√ß√£o que gera observa√ß√µes de uma distribui√ß√£o weibull, onde a distribui√ß√£o Weibull s√≥ tem um par√¢metro. Consulte detalhes em <https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html>.

No [link](https://numpy.org/doc/stable/reference/random/generated/numpy.random.weibull.html) voc√™ ver√° que o que √© implementado pelo m√©todo `random.weibull` √© gerar observa√ß√µes de $X = [-\log(U)]^{1/\alpha} \sim Weibull(\alpha)$, com $U$ sendo um v.a. uniforme no intervalo (0,1\] . Da√≠, para gerar observa√ß√µes da distribui√ß√£o $Weibull(\alpha, \beta)$, teremos que multiplicar o resultado de `random.weibull` pelo valor de $\beta$. Por√©m, com pouco c√≥digo, podemos construir uma fun√ß√£o para gerar observa√ß√µes da $Weibull(\alpha, \beta)$.

Veja como seria em R, Julia e Python:

::: panel-tabset
## R

```{r}
set.seed(0)
rweibull(n = 10L, shape = 2.5, scale = 1.5)
```

## Julia

```{julia}
using Distributions
using Random

Random.seed!(0);
rand(Weibull(2.5,1.5), 10)
```

## Python

```{python}
import numpy as np

def random_weibull(n, alpha, beta):
  return beta * np.random.weibull(alpha, n)

np.random.seed(0)

random_weibull(n = 10, alpha = 2.5, beta = 1.5)
```
:::

![Eu olhando param um pouco de malabarismo de c√≥digo (desnecess√°rio) em Python.](nao_entendi.gif)

::: callout-tip
# Isso √© um pouco estranho, mas tudo bem, sabemos programar!

Ter que multiplicar as observa√ß√µes geradas de uma distribui√ß√£o que deveria ter, em sua defini√ß√£o, dois par√¢metros me parece estranho! Perceba que no c√≥digo de Python foi preciso fazer definir a fun√ß√£o `random_weibull`, em que foi preciso considerar `beta * np.random.weibull(alpha, n)` para se ter observa√ß√µes Weibull com valores de $\beta$ diferente de 1. √â f√°cil adaptar, mais o designer n√£o √© legal, na minha opini√£o.

Afinal de contas, √© muito mais conveniente alterar o comportamento e resultados de uma fun√ß√£o passando argumentos para a fun√ß√£o, e n√£o fazendo as altera√ß√µes fora dela. √â esse o papel dos argumentos, n√£o? Se o usu√°rio tivesse interesse que $\beta = 1$, como ocorre em `random.weibull` ele poderia especificar isso como argumento da fun√ß√£o, certo?

Comportamentos mais convenientes s√£o observados em R e Julia, afinal de contas, elas surgiram com o foco na computa√ß√£o cient√≠fica. R √© mais voltada para ci√™ncia de dados e aprendizagem de m√°quina. J√° Julia, al√©m dos mesmos focos de R, tamb√©m √© uma linguagem de propr√≥sito geral, assim como Python √© em sua ess√™ncia.

Note que essa minha cr√≠tica n√£o √© a linguagem Python. Refere-se t√£o somente ao m√©todo `random.weibull` e alguns outros que seguem esse designer de implementa√ß√£o. Python √© uma √≥tima linguagem que vem melhorando o seu desempenho nas novas vers√µes. Veja as [novidades](https://docs.python.org/3.11/whatsnew/3.11.html) de lan√ßamento do Python 3.11, que alcan√ßou melhorias no desempenho computacional entre 10-60% quando comparado com Python 3.10. Algo em torno de 1.25x de aumento no desempenho, considerando o conjunto de benchmarks padr√£o que o comit√™ de desenvolvimento da linguagem utiliza.
:::

Agora sim, vamos aos tr√™s c√≥digos completos para a solu√ß√£o do problema que motiva o t√≠tulo desse post.

![](preguica_feliz.gif)

Nas tr√™s linguagens, utilizarei os mesmo conceitos importantes de implementa√ß√£o que conduzem a c√≥digos mais generalizados e a um melhor reaproveitamente de c√≥digo:

1.  Note que utilizo o conceito de fun√ß√µes com argumentos vari√°dicos, tamb√©m chamadas de **fun√ß√µes varargs**. Perceba que a fun√ß√£o `log_likelihood` n√£o precisa ser reimplementada novamente para outras fun√ß√µes densidades. A fun√ß√£o densidade de probabilidade √© um argumento dessa fun√ß√£o que denominei de `log_likelihood` nos tr√™s c√≥digos (R, Python e Julia). Precisamos de operadores **varargs**, tendo em vista que n√£o conhecemos o n√∫mero de par√¢metros da fdp que o usu√°rio ir√° passar como argumento. Fun√ß√µes com argumentos **varargs** √© uma t√©cnica muito poderosa. Utilizei esses conceitos em Python e Julia, tendo em vista que n√£o se faz necess√°rio para a fun√ß√£o `optim` em R. ‚ö°

2.  Note que n√£o √© preciso obter analiticamente a express√£o da fun√ß√£o de log-verossimilhan√ßa. N√£o h√° sentido nisso, tendo em vista que o nosso objetivo √© simplesmente obter as estimativas num√©ricas para $\alpha$ e $\beta$ . √â muito mais √∫til ter uma fun√ß√£o gen√©rica que se adeque a diversas outras situa√ß√µes!

3.  Outro conceito poderoso e que te leva a implementa√ß√µes consistentes √© entender o funcionamento das fun√ß√µes an√¥nimas, tamb√©m conhecida como [**fun√ß√µes lambda**](https://en.wikipedia.org/wiki/Anonymous_function)**.

4.  Foi utilizado como m√©todo de otimiza√ß√£o (minimiza√ß√£o), o m√©todo [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm). Escrevi a respeito dos m√©todos de quasi-Newton, classe de algoritmos que o m√©todo de **B**royden--**F**letcher--**G**oldfarb--**S**hanno - **BFGS** pertencem, nos materiais que diponibilizo aos meus alunos na discipina de estat√≠stica computacional que leciono no Departamento de Estat√≠stica da UFPB. Se quiser um pouco mais de detalhes, clique [aqui](https://prdm0.github.io/aulas_computacional/t%C3%B3picos-em-estat%C3%ADstica-computacional.html#otimiza%C3%A7%C3%A3o-n%C3%A3o-linear).

::: callout-tip
# Minimizar ou maximizar?

Alguns algoritmos de otimiza√ß√£o s√£o definidos para minimizar uma fun√ß√£o objetivo, como √© o caso da maioria das implementa√ß√µes dos m√©todos de busca global, onde se encaixa o m√©todo BFGS. Mas n√£o tem problema, uma vez que minimizar, $-f$ equivale a maximizar $f$, em que $f$ √© uma dada fun√ß√£o objetivo.
:::

::: panel-tabset
## R
```{r}
# Quantidade de observa√ß√µes
n <- 250L

# Par√¢metros que especificam a distribui√ß√£o verdadeira, i.e., distribui√ß√£o
# da vari√°vel aleat√≥ria cujo os dados s√£o observa√ß√µes.
alpha <- 2.5
beta <- 1.5

# Fixando uma semente para o gerador de n√∫meros pseudo-aleat√≥rios.
# Assim, conseguimos, toda vez que rodamos o c√≥digo, reproduzir 
# os mesmos dados.
set.seed(3)

# Gerando as observa√ß√µes. Esse ser√° o conjunto de dados que voc√™ tera para 
# modelar.
dados <- rweibull(n = n, shape = alpha, scale = beta)

pdf_weibull <- function(x, par){
  alpha <- par[1]
  beta <- par[2]
  alpha/beta * (x/beta)^(alpha-1) * exp(-(x/beta)^alpha)
}

# Checando se a densidade de pdf_weibull integra em 1
integrate(f = pdf_weibull, lower = 0, upper = Inf, par = c(2.5, 1.5))

# Em R, o operador dot-dot-dot (...) √© utilizado para definir
# quantidade vari√°dica de argumentos. Assim, log_likelihood √©
# uma fun√ß√£o vararg.
log_likelihood <- function(x, pdf, par)
  -sum(log(pdf(x, par)))

result <- optim(
  fn = log_likelihood,
  par = c(0.5, 0.5),
  method = "BFGS",
  x = dados,
  pdf = pdf_weibull
)

# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa
cat("Valores estimados de alpha e beta\n")
cat("--> alpha: ", result$par[1], "\n")
cat("--> beta: ", result$par[2], "\n")
```


## Python

```{python}
import numpy as np
import scipy.stats as stat
import scipy.integrate as inte
import scipy.optimize as opt

# Valores da distribui√ß√£o verdadeira
alpha = 2.5
beta = 1.5

# N√∫mero de observa√ß√µes que ir√£o compor nossos dados
n = 250 

# Implementando a fun√ß√£o random_weibull, em que os par√¢metros
# que indexam a distribui√ß√£o s√£o argumentos da fun√ß√£o. Tem mais
# sentido ser assim, n√£o?
def random_weibull(n, alpha, beta):
  return beta * np.random.weibull(alpha,n)

# Escrevendo a fun√ß√£p densidade de probabilidade da Weibull
# na reparametriza√ß√£o correta.
def pdf_weibull(x, param):
  alpha = param[0]
  beta = param[1]
  return alpha/beta * (x/beta)**(alpha-1) * np.exp(-(x/beta)**alpha)

# Testando se a densidade integra em 1
round(inte.quad(lambda x, alpha, beta: pdf_weibull(x, param = [alpha, beta]),
      0, np.inf, args = (1,1))[0],2)

# Implementando uma fun√ß√£o gen√©rica que implementa a fun√ß√£o objetivo
# (fun√ß√£o de log-verossimilhan√ßa) que iremos maximizar. Essa fun√ß√£o 
# ir√° receber como argumento uma fun√ß√£o densidade de probabilidade.
# N√£o √© preciso destrinchar (obter de forma exata) a fun√ß√£o de 
# log-verossimilhan√ßa!
# A fun√ß√£o de log-verossimilhan√ßa encontra-se multiplicada por -1
# devido ao fato da fun√ß√£o que iremos fazer otimiza√ß√£o minimizar 
# uma fun√ß√£o fun√ß√£o objetivo. Minimizar -f equivale a maximizar f.
# Lembre-se disso!
def log_likelihood(x, pdf, *args):
  return -np.sum(np.log(pdf(np.array(x), *args)))

# Gerando um conjunto de dados com alpha = 2.5 e beta = 1.5. Essa 
# √© nossa distribui√ß√£o verdadeira, i.e., √© a distribui√ß√£o que gera
# que gerou os dados que desejamos ajustar.
# Precisamos fixar uma semente, uma vez que queremos os mesmos dados
# toda vez que rodamos esse c√≥digo. 
np.random.seed(0)
dados = random_weibull(n = n, alpha = alpha, beta = beta)

# Miminimizando a fun√ß√£o -1 * log_likelihood, i.e., maximizando
# a fun√ß√£o log_likelihood.
alpha, beta = np.round(opt.minimize(
  fun = lambda *args: log_likelihood(dados, pdf_weibull, *args),
  x0=[0.5, 0.5]
).x, 2)

# Imprimindo os valores das estimativas de m√°xima verossimilhan√ßa
print("Valores estimados de alpha e beta\n")
print("--> alpha: ", alpha, "\n")
print("--> beta: ", beta, "\n")
```
:::

## Conclus√µes
